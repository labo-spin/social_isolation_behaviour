{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_window = (4000,5200) #es la ventana de tiempo del vdeo que quiero analizar, el original estaba de 100-5500.\n",
    "res_dir = r\"D:\\Valen Azar\\Escape\\Aislados\\Analisis_Aislados\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c69061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Replace these with your actual directories\n",
    "csv_directory = r'D:\\Valen Azar\\Escape\\Aislados\\Analisis_Aislados' #la dirección donde están los csv\n",
    "datos_directory = r'D:\\Valen Azar\\Escape\\Aislados\\Analisis_Aislados'\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "all_data = []\n",
    "\n",
    "# Read Datos Excel file and keep only relevant columns\n",
    "datos_path = os.path.join(datos_directory, 'Datos_Todos.xlsx')\n",
    "datos_df = pd.read_excel(datos_path)\n",
    "\n",
    "# Go through each CSV file in the directory\n",
    "for filename in os.listdir(csv_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract video name from filename\n",
    "        video_name = filename.split('DLC')[0] + \"DLC_resnet50_Arena Escape ActualizadaApr22shuffle1_300000.csv\"\n",
    "        \n",
    "\n",
    "        # Find the metadata for this video in the Datos DataFrame\n",
    "        metadata = datos_df[datos_df['Video'] == video_name].iloc[0].to_dict()\n",
    "\n",
    "        # Read the CSV file without header information\n",
    "        csv_path = os.path.join(csv_directory, filename)\n",
    "        temp_df = pd.read_csv(csv_path, header=None, skiprows=[0])\n",
    "\n",
    "        # Create new column names based on the second and third rows from the CSV\n",
    "        # (which are now the first and second rows in temp_df since we skipped the original first row)\n",
    "        new_columns = [f\"{a}_{b}\" for a, b in zip(temp_df.iloc[0], temp_df.iloc[1])]\n",
    "\n",
    "        # Drop the first two rows, which are now part of the column names\n",
    "        temp_df = temp_df.drop([0, 1]).reset_index(drop=True)\n",
    "\n",
    "        # Assign the new column names to temp_df\n",
    "        temp_df.columns = new_columns\n",
    "\n",
    "        # Drop columns that have 'likelihood' in their name\n",
    "        temp_df = temp_df[[col for col in temp_df.columns if 'likelihood' not in col]]\n",
    "\n",
    "        # Create a dictionary to hold DataFrame and metadata\n",
    "        video_dict = {\n",
    "            'video_name': video_name,\n",
    "            'tracking_data': temp_df,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        all_data.append(video_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d574c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all_data is populated\n",
    "if len(all_data) == 0:\n",
    "    print(\"Error: all_data list is empty.\")\n",
    "else:\n",
    "    print(f\"Successfully processed {len(all_data)} CSV files.\")\n",
    "\n",
    "# Loop through each dictionary in all_data to validate its contents\n",
    "for i, video_dict in enumerate(all_data):\n",
    "    print(f\"Checking dictionary {i + 1}...\")\n",
    "\n",
    "    # Check video_name\n",
    "    if not isinstance(video_dict.get('video_name', None), str):\n",
    "        print(f\"  Error: video_name in dictionary {i + 1} is not a string.\")\n",
    "\n",
    "    # Check tracking_data\n",
    "    tracking_data = video_dict.get('tracking_data', None)\n",
    "    if tracking_data is None or not isinstance(tracking_data, pd.DataFrame) or tracking_data.empty:\n",
    "        print(f\"  Error: tracking_data in dictionary {i + 1} is not a valid DataFrame.\")\n",
    "\n",
    "    # Check metadata\n",
    "    metadata = video_dict.get('metadata', None)\n",
    "    if metadata is None or not all(key in metadata for key in ['Video', 'Visual', 'Auditivo']):\n",
    "        print(f\"  Error: metadata in dictionary {i + 1} is missing one or more keys.\")\n",
    "    else:\n",
    "        print(f\"  Metadata for {video_dict['video_name']}: {metadata}\")\n",
    "\n",
    "print(\"Check complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19922399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store trials grouped by visual and auditory intensities\n",
    "trials = {}\n",
    "\n",
    "# Iterate over each video_dict in all_data\n",
    "for video_dict in all_data:\n",
    "    # Extract visual and auditory intensities from the metadata\n",
    "    visual_intensity = video_dict['metadata']['Visual']\n",
    "    auditory_intensity = video_dict['metadata']['Auditivo']\n",
    "    \n",
    "    # Create a key based on the combination of visual and auditory intensities\n",
    "    key = f\"{visual_intensity}_{auditory_intensity}\"\n",
    "    \n",
    "    # Add the video_dict to the corresponding entry in the trials dictionary\n",
    "    if key not in trials:\n",
    "        trials[key] = []\n",
    "    trials[key].append(video_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "import math\n",
    "\n",
    "def vector_magnitude(v):\n",
    "    return math.sqrt(v[0]**2 + v[1]**2)\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    return v1[0]*v2[0] + v1[1]*v2[1]\n",
    "\n",
    "def angle_between_vectors(v1, v2):\n",
    "    # Handle zero vectors\n",
    "    if vector_magnitude(v1) == 0 or vector_magnitude(v2) == 0:\n",
    "        return None  # Angle is undefined for zero vectors\n",
    "\n",
    "    # Calculate cosine of the angle\n",
    "    cos_theta = dot_product(v1, v2) / (vector_magnitude(v1) * vector_magnitude(v2))\n",
    "    \n",
    "    # Clip the value to the range [-1, 1] to handle any possible rounding errors\n",
    "    cos_theta = min(1.0, max(-1.0, cos_theta))\n",
    "    \n",
    "    # Calculate the angle in radians\n",
    "    angle = math.acos(cos_theta)\n",
    "    \n",
    "    # Convert the angle to degrees for easier interpretation\n",
    "    angle_degrees = math.degrees(angle)\n",
    "    \n",
    "    return angle_degrees\n",
    "\n",
    "def calculate_features(df):\n",
    "    # Convert columns to numeric type if they are not already\n",
    "    numeric_columns = [\n",
    "        'ojo izquierdo_x', 'ojo izquierdo_y', 'ojo derecho_x', 'ojo derecho_y', \n",
    "        'vejiga anterior_x', 'vejiga anterior_y', 'vejiga posterior_x', 'vejiga posterior_y', \n",
    "        'cola 1_x', 'cola 1_y', 'cola 2_x', 'cola 2_y', \n",
    "        'cola 3_x', 'cola 3_y', 'cola 4_x', 'cola 4_y', \n",
    "        'cola 5_x', 'cola 5_y', 'cola 6_x', 'cola 6_y'\n",
    "    ]\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    \n",
    "    # Calculate the mean point between both eyes\n",
    "    df['eye_mean_x'] = (df['ojo izquierdo_x'] + df['ojo derecho_x']) / 2\n",
    "    df['eye_mean_y'] = (df['ojo izquierdo_y'] + df['ojo derecho_y']) / 2\n",
    "    \n",
    "    # Calculate the mean point between the last two tail points\n",
    "    df['tail_mean_x'] = (df['cola 5_x'] + df['cola 6_x']) / 2\n",
    "    df['tail_mean_y'] = (df['cola 5_y'] + df['cola 6_y']) / 2\n",
    "    \n",
    "    # Calculate Instant head and tail velocity\n",
    "    df['head_velocity'] = np.sqrt((df['eye_mean_x'].diff())**2 + (df['eye_mean_y'].diff())**2)\n",
    "    df['tail_velocity'] = np.sqrt((df['tail_mean_x'].diff())**2 + (df['tail_mean_y'].diff())**2)\n",
    "    \n",
    "    # Calculate Angular Velocity\n",
    "    current_head_vector = [df[\"vejiga posterior_x\"] - df[\"eye_mean_x\"], df[\"vejiga posterior_y\"] - df[\"eye_mean_y\"]]\n",
    "    current_head_vector = np.asarray(current_head_vector).astype(\"float\")\n",
    "    current_head_vector = current_head_vector.T\n",
    "    \n",
    "    # Calculate the magnitudes of these vectors\n",
    "    magnitudes = np.linalg.norm(current_head_vector, axis=1)\n",
    "\n",
    "    # Calculate the dot product between each pair of subsequent vectors\n",
    "    dot_products = np.sum(current_head_vector[:-1] * current_head_vector[1:], axis=1)\n",
    "\n",
    "    # Calculate the magnitudes of pairs of subsequent vectors\n",
    "    magnitude_pairs = magnitudes[:-1] * magnitudes[1:]\n",
    "\n",
    "    # Calculate the angles in radians\n",
    "    angles = np.arccos(dot_products / magnitude_pairs)\n",
    "    angles = np.concatenate([angles, [angles[-1]]])\n",
    "    \n",
    "    # Calculate the angle between the current vector and the previous vector\n",
    "    df['angular_velocity'] = angles\n",
    "    \n",
    "    # Calculate Sum of Tail Angles\n",
    "    tail_angles = []\n",
    "    for i in range(1, 5):\n",
    "        vec1 = df[f'cola {i}_x'] - df[f'cola {i+1}_x'], df[f'cola {i}_y'] - df[f'cola {i+1}_y']\n",
    "        vec2 = df[f'cola {i+1}_x'] - df[f'cola {i+2}_x'], df[f'cola {i+1}_y'] - df[f'cola {i+2}_y']\n",
    "        angle = np.arctan2(vec2[1], vec2[0]) - np.arctan2(vec1[1], vec1[0])\n",
    "        tail_angles.append(angle)\n",
    "    df['sum_tail_angles'] = np.sum(tail_angles, axis=0)\n",
    "    \n",
    "    # Calculate Instant velocity for tail6 point\n",
    "    df['tail6_velocity'] = np.sqrt((df['cola 3_x'].diff())**2 + (df['cola 3_y'].diff())**2)\n",
    "    \n",
    "    df['head_tail_distance'] = np.sqrt((df['eye_mean_x'] - df['tail_mean_x'])**2 + (df['eye_mean_y'] - df['tail_mean_y'])**2)\n",
    "    \n",
    "    df['head_acceleration'] = df['head_velocity'].diff().fillna(0)\n",
    "    df['tail_acceleration'] = df['tail_velocity'].diff().fillna(0)\n",
    "\n",
    "    \n",
    "    df['angular_acceleration'] = df['angular_velocity'].diff().fillna(0)\n",
    "\n",
    "    \n",
    "    df['head_jerk'] = df['head_acceleration'].diff().fillna(0)\n",
    "    df['tail_jerk'] = df['tail_acceleration'].diff().fillna(0)\n",
    "\n",
    "    \n",
    "    curvatures = []\n",
    "    for i in range(1, 5):\n",
    "        x1, y1 = df[f'cola {i}_x'], df[f'cola {i}_y']\n",
    "        x2, y2 = df[f'cola {i+1}_x'], df[f'cola {i+1}_y']\n",
    "        x3, y3 = df[f'cola {i+2}_x'], df[f'cola {i+2}_y']\n",
    "\n",
    "        numerator = (x1 - x2) * (y2 - y3) - (y1 - y2) * (x2 - x3)\n",
    "        denominator = np.sqrt(((x1 - x2)**2 + (y1 - y2)**2) * ((x2 - x3)**2 + (y2 - y3)**2) * ((x1 - x3)**2 + (y1 - y3)**2))\n",
    "\n",
    "        curvature = numerator / denominator\n",
    "        curvatures.append(curvature)\n",
    "\n",
    "    df['sum_curvature'] = np.sum(curvatures, axis=0)\n",
    "\n",
    "    # Compute the velocity vector components\n",
    "    delta_x = df['eye_mean_x'].diff()\n",
    "    delta_y = df['eye_mean_y'].diff()\n",
    "\n",
    "    # Compute the heading vector components\n",
    "    heading_x = df['vejiga posterior_x'] - df['eye_mean_x']\n",
    "    heading_y = df['vejiga posterior_y'] - df['eye_mean_y']\n",
    "\n",
    "    # Compute the angle\n",
    "    df['angle_velocity_heading'] = np.arccos((delta_x * heading_x + delta_y * heading_y) / \n",
    "                                              (np.sqrt(delta_x**2 + delta_y**2) * np.sqrt(heading_x**2 + heading_y**2)))\n",
    "    \n",
    "    df['angle_velocity_heading'] = df['angle_velocity_heading'].fillna(0)\n",
    "    \n",
    "    df['curvature_rate'] = df['sum_curvature'].diff().fillna(0)\n",
    "\n",
    "    return df[['head_velocity', 'tail_velocity', 'angular_velocity',\n",
    "               'sum_tail_angles', 'tail6_velocity',\n",
    "               'head_tail_distance', 'head_acceleration', 'tail_acceleration',\n",
    "               'angular_acceleration', 'head_jerk', 'tail_jerk', 'sum_curvature',\n",
    "               'angle_velocity_heading', 'curvature_rate']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "threshold_probability = 0.000000000000000001  # You can change this threshold later\n",
    "\n",
    "# Calculate features and find events for each trial\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        tracking_data = trial['tracking_data']\n",
    "        new_features_df = calculate_features(tracking_data)\n",
    "        \n",
    "        # Step 1: Calculate the median velocity\n",
    "        median_velocity = new_features_df['head_velocity'].median()\n",
    "        \n",
    "        # Step 2: Create a mirrored distribution around the median\n",
    "        lower_half = new_features_df['head_velocity'][new_features_df['head_velocity'] < median_velocity]\n",
    "        mirrored_distribution = np.concatenate([lower_half, 2 * median_velocity - lower_half])\n",
    "        \n",
    "        # Step 3: Calculate mean and standard deviation\n",
    "        mu, std = np.mean(mirrored_distribution), np.std(mirrored_distribution)\n",
    "        \n",
    "        # Step 4: Calculate probability for each timepoint\n",
    "        cdf_values = norm.cdf(new_features_df['head_velocity'], mu, std)\n",
    "        probabilities = 1 - cdf_values  # Tail probabilities\n",
    "        \n",
    "        # Step 5: Identify events\n",
    "        # Initialize variables\n",
    "        in_event = False\n",
    "        event_times = []\n",
    "        event_ends = []\n",
    "        current_event_start = None\n",
    "        above_threshold_counter = 0  # Initialize counter\n",
    "        n = 1  # ERA 3 #Number of frames needed to confirm end of event #era 20 #Esto quiere decir que si yo tengo al pez que se mueve con distinta velocidad, cuando la misma traspasa cierto umbral, para que sea un evento tiene que estar como minimo 5 frames. Cuando vuelve a traspasar el umbral, es un nuevo evento.\n",
    "        velocity_threshold = 1  # Replace with your specific value #era 1 #Umbral de velocidad para considerar un evento.\n",
    "        event_valid = False  # Initialize event validity flag\n",
    "\n",
    "        for i, prob in enumerate(probabilities):\n",
    "            velocity = new_features_df['head_velocity'].iloc[i]\n",
    "\n",
    "            if prob < threshold_probability:\n",
    "                above_threshold_counter = 0  # Reset counter\n",
    "                if not in_event:\n",
    "                    in_event = True\n",
    "                    event_valid = False  # Reset event validity flag\n",
    "                    current_event_start = i  # Mark the start of the new event\n",
    "\n",
    "            if in_event:\n",
    "                if velocity > velocity_threshold:\n",
    "                    event_valid = True  # Mark event as valid\n",
    "\n",
    "            if prob >= threshold_probability:\n",
    "                if in_event:\n",
    "                    above_threshold_counter += 1  # Increment counter\n",
    "                    if above_threshold_counter >= n:\n",
    "                        in_event = False\n",
    "                        above_threshold_counter = 0  # Reset counter\n",
    "\n",
    "                        if event_valid:  # Only add event if it's valid\n",
    "                            event_ends.append(i)  # Mark the end of the event\n",
    "                            event_times.append(current_event_start)  # Save the start time of the event\n",
    "\n",
    "        # Save features and event times\n",
    "        trial['features'] = new_features_df\n",
    "        trial['event_times'] = event_times # esto es el frame de inicio de un evento\n",
    "        trial['event_ends'] = event_ends # esto es el frame final de un evento. Cuando hago event_ends - event_times, me da la duración del evento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07605ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualización del resultado\n",
    "back_window = 5\n",
    "forw_window = 25\n",
    "x_limit = 5500  # Límite del eje X\n",
    "y_limit = 12  # Definimos el límite en el eje Y\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    random_trials = sample(trial_list, min(5, len(trial_list)))\n",
    "    \n",
    "    for i, trial in enumerate(random_trials):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        features_df = trial['features']\n",
    "        event_times = trial['event_times']\n",
    "        \n",
    "        for event_time in event_times:\n",
    "            if event_time <= x_limit:  # Solo mostrar eventos dentro del rango\n",
    "                maxvel = features_df['head_velocity'][event_time - back_window: event_time + forw_window].max()\n",
    "                if maxvel > 4:\n",
    "                    plt.axvline(x=event_time, color='red', linestyle='--', linewidth=2, label='Event' if event_time == event_times[0] else \"\")\n",
    "                    plt.text(event_time, maxvel, str(event_time), color='red', fontsize=12, ha='center')\n",
    "\n",
    "        plt.plot(features_df['head_velocity'], label='Instant Head Velocity')\n",
    "        plt.title(f'Stimulus: {stimulus}, Trial: {trial[\"video_name\"]}')\n",
    "        plt.xlabel('Timepoint')\n",
    "        plt.ylabel('Normalized Instant Head Velocity')\n",
    "        plt.legend()\n",
    "        plt.xlim(0, x_limit)  # Establecer el límite del eje X\n",
    "        plt.ylim(0, y_limit)  # Ajustar el límite del eje Y a 12\n",
    "        plt.savefig(f\"Event_example_{trial['video_name']}.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3fb0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = trials[\"160_0.0\"][0] #acá voy variando entre las combinaciones de estímulos (160-0);(230-0) etc.\n",
    "features = trial['features'] #las features son las variables que el codigo calcula con el csv y que están detalladas más arriba.\n",
    "feature_list = []\n",
    "for feature_name, feature_values in features.items():\n",
    "    feature_list.append(feature_name)\n",
    "    print(feature_name) #me da los nombres de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe4d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#back_window = 20\n",
    "#forw_window = 40\n",
    "\n",
    "back_window = 5\n",
    "forw_window = 25\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        features = trial['features']\n",
    "        event_times = trial['event_times']\n",
    "        \n",
    "        event_features_over_time = []\n",
    "        valid_event_times = []\n",
    "        \n",
    "        for event_time in event_times:\n",
    "            \n",
    "            # Exclude events too close to the trial limits\n",
    "            if event_time < back_window or event_time > len(features['head_velocity']) - forw_window:\n",
    "                continue\n",
    "            \n",
    "            maxvel = features['head_velocity'][event_time - back_window: event_time + forw_window].max()\n",
    "            \n",
    "            if maxvel < 3 or maxvel > 25:   #velocidad original entre < 3 y > 15.\n",
    "                continue\n",
    "            \n",
    "            valid_event_times.append(event_time)\n",
    "            time_window_features = []\n",
    "            \n",
    "            for feature_name, feature_values in features.items():\n",
    "                # Extract 20 frames before to 40 frames after the event\n",
    "                time_window = feature_values[event_time - back_window: event_time + forw_window]\n",
    "                time_window_features.append(time_window)\n",
    "            \n",
    "            event_features_over_time.append(time_window_features)\n",
    "        \n",
    "        trial['event_features_over_time'] = np.array(event_features_over_time)\n",
    "        trial['valid_event_times'] = np.array(valid_event_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af682c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize an empty list to store max head velocities\n",
    "# max_head_velocities = []\n",
    "\n",
    "# # Loop through each trial based on the stimulus types\n",
    "# for stimulus_type, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         # Access the 'event_features_over_time' array for the trial\n",
    "#         event_features_over_time = trial['event_features_over_time']\n",
    "        \n",
    "#         head_velocity_idx = feature_list.index('angular_acceleration')\n",
    "        \n",
    "#         # Loop through each event and find the maximum head velocity\n",
    "#         for event in range(event_features_over_time.shape[0]):\n",
    "#             max_head_velocity = event_features_over_time[event, head_velocity_idx, :].max()\n",
    "#             max_head_velocities.append(max_head_velocity)\n",
    "\n",
    "# # Create the histogram\n",
    "# plt.hist(max_head_velocities, bins=50)\n",
    "# plt.title('Histogram of Max Head Velocities')\n",
    "# plt.xlabel('Max Head Velocity')\n",
    "# plt.ylabel('Frequency')\n",
    "# #plt.ylim(0,400)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d82b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['head_velocity', 'angular_velocity', 'head_tail_distance', 'sum_curvature']\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        event_features = []\n",
    "        \n",
    "        for event_time_window in trial['event_features_over_time']:\n",
    "            event_feature_params = []\n",
    "            max_frame_diffs = []  # List to store the differences in frame numbers for the selected features\n",
    "            \n",
    "            feature_to_max_frame = {}  # Dictionary to store the frame of max value for each feature\n",
    "            \n",
    "            for i, feature_name in enumerate(trial['features'].keys()):\n",
    "                time_window = event_time_window[i]\n",
    "                \n",
    "                # Your existing code to calculate event_feature_params\n",
    "                if feature_name == 'head_tail_distance':\n",
    "                    event_feature_params.extend([np.min(time_window), np.mean(time_window)])\n",
    "                elif feature_name in ['sum_tail_angles', 'head_acceleration', 'tail_acceleration', \n",
    "                                      'angular_acceleration', 'head_jerk', 'tail_jerk', \n",
    "                                      'sum_curvature', 'curvature_rate']:\n",
    "                    event_feature_params.extend([np.max(time_window) - np.min(time_window), \n",
    "                                                 np.max(np.abs(time_window)), \n",
    "                                                 np.mean(np.abs(time_window))])\n",
    "                else:\n",
    "                    event_feature_params.extend([np.max(time_window), np.mean(time_window)])\n",
    "                \n",
    "                # Check if the feature is one of the selected features\n",
    "                if feature_name in selected_features:\n",
    "                    max_frame = np.argmax(time_window)  # Find the frame where the max value occurs\n",
    "                    feature_to_max_frame[feature_name] = max_frame  # Store it in the dictionary\n",
    "            \n",
    "            # Calculate the differences in max frame numbers for each pair of selected features\n",
    "            for i, feature1 in enumerate(selected_features):\n",
    "                for j, feature2 in enumerate(selected_features[i+1:]):\n",
    "                    max_frame_diff = abs(feature_to_max_frame[feature1] - feature_to_max_frame[feature2])\n",
    "                    max_frame_diffs.append(max_frame_diff)\n",
    "            \n",
    "            # Append both the existing and new parameters for this event\n",
    "            event_feature_params.extend(max_frame_diffs)\n",
    "            event_features.append(event_feature_params)\n",
    "        \n",
    "        # Update the event_features array for the current trial\n",
    "        trial['event_features'] = np.array(event_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stimulus, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         event_features = []\n",
    "        \n",
    "#         for event_time_window in trial['event_features_over_time']:\n",
    "#             event_feature_params = []\n",
    "            \n",
    "#             for i, feature_name in enumerate(trial['features'].keys()):\n",
    "#                 time_window = event_time_window[i]\n",
    "                \n",
    "#                 if feature_name == 'head_tail_distance':\n",
    "#                     event_feature_params.extend([np.min(time_window), np.mean(time_window)])\n",
    "#                 elif feature_name in ['sum_tail_angles', 'head_acceleration', 'tail_acceleration', \n",
    "#                                       'angular_acceleration', 'head_jerk', 'tail_jerk', \n",
    "#                                       'sum_curvature', 'curvature_rate']:\n",
    "#                     event_feature_params.extend([np.max(time_window) - np.min(time_window), \n",
    "#                                                  np.max(np.abs(time_window)), \n",
    "#                                                  np.mean(np.abs(time_window))])\n",
    "#                 else:\n",
    "#                     event_feature_params.extend([np.max(time_window), np.mean(time_window)])\n",
    "            \n",
    "#             event_features.append(event_feature_params)\n",
    "        \n",
    "#         trial['event_features'] = np.array(event_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a92452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize an empty dictionary to collect frame differences for each pair of features\n",
    "# frame_diffs_dict = {}\n",
    "\n",
    "# for stimulus, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         # Assuming the last N columns in event_features are the frame differences,\n",
    "#         # where N is the number of combinations of selected features\n",
    "#         num_combinations = len(selected_features) * (len(selected_features) - 1) // 2\n",
    "#         if trial['event_features'].shape[0] > 0:\n",
    "#             frame_diffs = trial['event_features'][:, -num_combinations:]\n",
    "\n",
    "#             # If this is the first trial, initialize the dictionary\n",
    "#             if not frame_diffs_dict:\n",
    "#                 idx = 0\n",
    "#                 for i, feature1 in enumerate(selected_features):\n",
    "#                     for feature2 in selected_features[i+1:]:\n",
    "#                         frame_diffs_dict[f\"{feature1}-{feature2}\"] = []\n",
    "#                         idx += 1\n",
    "\n",
    "#             # Append the frame differences to the respective lists\n",
    "#             for idx, feature_pair in enumerate(frame_diffs_dict.keys()):\n",
    "#                 feature1, feature2 = feature_pair.split('-')\n",
    "#                 frame_diffs_dict[f\"{feature1}-{feature2}\"].extend(frame_diffs[:, idx])\n",
    "\n",
    "# # Plotting the histograms\n",
    "# for feature_pair, diff_values in frame_diffs_dict.items():\n",
    "#     plt.figure()\n",
    "#     plt.hist(diff_values, bins=15)\n",
    "#     plt.title(f\"Frame Differences for {feature_pair}\")\n",
    "#     plt.xlabel(\"Frame Difference\")\n",
    "#     plt.ylabel(\"Frequency\")\n",
    "#     plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = []\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        all_events.extend(trial['event_features'])\n",
    "\n",
    "all_events_array = np.array(all_events)\n",
    "\n",
    "# Normalize\n",
    "mean_vals = np.mean(all_events_array, axis=0)\n",
    "std_vals = np.std(all_events_array, axis=0)\n",
    "normalized_events = (all_events_array - mean_vals) / std_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_events.shape #me da el número de eventos (519) y el número de dimensiones (42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opciones: TSNE, PCA y KernelPCA. La idea es ir jugando con los parametros de tal forma que se observen clusters. \n",
    "# Los puntos rojos son los eventos que yo identifiqué, si en los gráficos se me juntan los puntos rojos es probable que \n",
    "# haya un cluster ahí.\n",
    "\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#tsne = TSNE(n_components=2, perplexity = 25)\n",
    "#low_dimensional_data = tsne.fit_transform(normalized_events)\n",
    "\n",
    "#from umap import UMAP\n",
    "\n",
    "#umap = UMAP(n_components=2, n_neighbors=20, min_dist=0.3)\n",
    "#low_dimensional_data = umap.fit_transform(normalized_events)\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "#pca = PCA(n_components=2) #el numero de componentes original es de 2.\n",
    "#low_dimensional_data = pca.fit_transform(normalized_events)\n",
    "\n",
    "from sklearn.decomposition import KernelPCA #EL QUE ESTUVE USANDO Y PUSE EN LA TESIS\n",
    "\n",
    "kernel_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma = 0.015) #Original gamma=0.015 # Tengo que jugar con el gamma para ver si se forman clusters o no. O se alejan o se juntan.\n",
    "low_dimensional_data = kernel_pca.fit_transform(normalized_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_counter = 0\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        num_events = len(trial['event_features'])\n",
    "        trial['low_D_representation'] = low_dimensional_data[index_counter:index_counter + num_events]\n",
    "        index_counter += num_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e78887",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        F_escape = trial['metadata']['F_escape']\n",
    "        valid_event_times = trial['valid_event_times']\n",
    "\n",
    "        is_escape = np.zeros(len(valid_event_times), dtype=bool)\n",
    "\n",
    "        if not np.isnan(F_escape) and len(valid_event_times) > 0:\n",
    "            closest_event = np.argmin(np.abs(valid_event_times - F_escape))\n",
    "            if np.abs(valid_event_times[closest_event] - F_escape) < 50:\n",
    "                is_escape[closest_event] = True\n",
    "\n",
    "        trial['is_escape'] = is_escape\n",
    "\n",
    "        trial['condicion'] = trial['metadata']['condicion'] #social o aislado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.savefig(f\"01 - Diff in N of Active Neurons and Diff in Mean Neural Act Over Time ({active_stim_code} - {ctrl_stim_code}).pdf\", format='pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARGA DE DATOS CON FILTRO # NO USAR POR EL MOMENTO, NO VAMOS A VER EN CADA ESTÍMULO\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "\n",
    "## Initialize empty lists to collect data points and colors\n",
    "#concatenated_low_D = []\n",
    "#concatenated_low_D_in_range = []\n",
    "#concatenated_colors = []\n",
    "#concatenated_colors_in_range = []\n",
    "\n",
    "## Iterate over trials and collect data points and colors\n",
    "#for stimulus, trial_list in trials.items():\n",
    "    \n",
    "    # Filtrar solo los estímulos \"160-0.0\"  \n",
    "    #if stimulus != \"160-0.0\":\n",
    "     #    continue\n",
    "    # Si cambio el 'and' por 'or' me va a dar todos los videos que contengan o 230 o 0. (original)\n",
    "    #if not (\"255\" in stimulus and \"0.04\" in stimulus):\n",
    "     #   continue\n",
    "        \n",
    "    #for trial in trial_list:\n",
    "     #   if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "      #      continue\n",
    "        # Si quiero puntos de los aislados solo pongo: if trial['condicion'] != 'aislado'. Si quiero aislados y sociales no corro esta parte (la pongo con asteriscos)\n",
    "        #if trial['condicion'] != 'social':\n",
    "         #   continue\n",
    "        \n",
    "       # within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1])\n",
    "\n",
    "        ## Data for first subplot (only within_frame_range)\n",
    "        #concatenated_low_D_in_range.extend(trial['low_D_representation'][within_frame_range])\n",
    "        #concatenated_colors_in_range.extend(['red' if escape and trial['condicion'] == 'social' else 'green' if escape and trial['condicion'] == 'aislado' else 'blue' for escape in trial['is_escape'][within_frame_range]])\n",
    "\n",
    "        ## Data for second subplot (all data)\n",
    "        #concatenated_low_D.extend(trial['low_D_representation'])\n",
    "        #concatenated_colors.extend(['red' if escape and trial['condicion'] == 'social' else 'green' if escape and trial['condicion'] == 'aislado' else 'blue' for escape in trial['is_escape']])\n",
    "        ## 'red' if escape and trial['condicion'] == 'social' else 'green' if escape and trial['condicion'] == 'aislado' else 'blue'. Sociales son rojos, aislados verdes, lo otro es azul que son cstart no anotados.\n",
    "\n",
    "## Convert lists to NumPy arrays for consistency\n",
    "#concatenated_low_D_in_range = np.array(concatenated_low_D_in_range)\n",
    "#concatenated_colors_in_range = np.array(concatenated_colors_in_range)\n",
    "#concatenated_low_D = np.array(concatenated_low_D)\n",
    "#concatenated_colors = np.array(concatenated_colors)\n",
    "\n",
    "## Check if lengths match (for debugging, can remove later)\n",
    "#assert len(concatenated_low_D) == len(concatenated_colors), \"Lengths must match!\"\n",
    "\n",
    "## Create subplots\n",
    "#fig, axs = plt.subplots(1, 2, figsize=(24, 12), sharey=True)\n",
    "\n",
    "## First subplot (only within_frame_range)\n",
    "#axs[0].scatter(concatenated_low_D_in_range[:, 0][concatenated_colors_in_range == \"blue\"], concatenated_low_D_in_range[:, 1][concatenated_colors_in_range == \"blue\"], color=\"blue\", alpha=0.3, s=10)\n",
    "#axs[0].scatter(concatenated_low_D_in_range[:, 0][concatenated_colors_in_range == \"red\"], concatenated_low_D_in_range[:, 1][concatenated_colors_in_range == \"red\"], color=\"red\", alpha=0.6, s=40)\n",
    "#axs[0].scatter(concatenated_low_D_in_range[:, 0][concatenated_colors_in_range == \"green\"], concatenated_low_D_in_range[:, 1][concatenated_colors_in_range == \"green\"], color=\"green\", alpha=0.6, s=40)\n",
    "#axs[0].set_title(\"Within Stim Range\", fontsize=16)\n",
    "#axs[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "## Second subplot (all data)\n",
    "#axs[1].scatter(concatenated_low_D[:, 0][concatenated_colors == \"blue\"], concatenated_low_D[:, 1][concatenated_colors == \"blue\"], color=\"blue\", alpha=0.3, s=10)\n",
    "#axs[1].scatter(concatenated_low_D[:, 0][concatenated_colors == \"red\"], concatenated_low_D[:, 1][concatenated_colors == \"red\"], color=\"red\", alpha=0.6, s=40)\n",
    "#axs[1].scatter(concatenated_low_D[:, 0][concatenated_colors == \"green\"], concatenated_low_D[:, 1][concatenated_colors == \"green\"], color=\"green\", alpha=0.6, s=40)\n",
    "#axs[1].set_title(\"All Events\", fontsize=16)\n",
    "#axs[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "##plt.savefig(f\"01 - 2D representation of escape events.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28aca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGA DE DATOS SIN FILTRO - Todos los animales aislados y sociales\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize empty lists to collect data points and colors\n",
    "concatenated_low_D = []\n",
    "concatenated_low_D_in_range = []\n",
    "concatenated_colors = []\n",
    "concatenated_colors_in_range = []\n",
    "\n",
    "# Iterate over trials and collect data points and colors. la parte del tipo de estimulo es nueva, lo de 'if not () continue'.\n",
    "for stimulus, trial_list in trials.items():\n",
    "    #if not (\"230\" in stimulus and \"0\" in stimulus):\n",
    "     #   continue\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "            # Si quiero puntos de los aislados solo pongo: if trial['condicion'] != 'aislado'. Si quiero aislados y sociales no corro esta parte (la pongo con numeral)\n",
    "        if trial['condicion'] != 'aislado':\n",
    "            continue\n",
    "        within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1])\n",
    "\n",
    "        # Data for first subplot (only within_frame_range)\n",
    "        concatenated_low_D_in_range.extend(trial['low_D_representation'][within_frame_range])\n",
    "        concatenated_colors_in_range.extend(['red' if escape and trial['condicion'] == 'social' else 'green' if escape and trial['condicion'] == 'aislado' else 'blue' for escape in trial['is_escape'][within_frame_range]])\n",
    "\n",
    "        # Data for second subplot (all data)\n",
    "        concatenated_low_D.extend(trial['low_D_representation'])\n",
    "        concatenated_colors.extend(['red' if escape and trial['condicion'] == 'social' else 'green' if escape and trial['condicion'] == 'aislado' else 'blue' for escape in trial['is_escape']])\n",
    "        # 'red' if escape and trial['condicion'] == 'social' else 'green' if escape and trial['condicion'] == 'aislado' else 'blue'. Sociales son rojos, aislados verdes, lo otro es azul que son cstart no anotados.\n",
    "\n",
    "# Convert lists to NumPy arrays for consistency\n",
    "concatenated_low_D_in_range = np.array(concatenated_low_D_in_range)\n",
    "concatenated_colors_in_range = np.array(concatenated_colors_in_range)\n",
    "concatenated_low_D = np.array(concatenated_low_D)\n",
    "concatenated_colors = np.array(concatenated_colors)\n",
    "\n",
    "# Check if lengths match (for debugging, can remove later)\n",
    "assert len(concatenated_low_D) == len(concatenated_colors), \"Lengths must match!\"\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(24, 12), sharey=True)\n",
    "\n",
    "# First subplot (only within_frame_range) - PARA SACAR LOS PUNTOS AZULES SOLO PONGO # ANTES DE AXS [0] PARA LOS BLUE\n",
    "#axs[0].scatter(concatenated_low_D_in_range[:, 0][concatenated_colors_in_range == \"blue\"], concatenated_low_D_in_range[:, 1][concatenated_colors_in_range == \"blue\"], color=\"blue\", alpha=0.3, s=10)\n",
    "axs[0].scatter(concatenated_low_D_in_range[:, 0][concatenated_colors_in_range == \"red\"], concatenated_low_D_in_range[:, 1][concatenated_colors_in_range == \"red\"], color=\"red\", alpha=0.6, s=40)\n",
    "axs[0].scatter(concatenated_low_D_in_range[:, 0][concatenated_colors_in_range == \"green\"], concatenated_low_D_in_range[:, 1][concatenated_colors_in_range == \"green\"], color=\"green\", alpha=0.6, s=40)\n",
    "axs[0].set_title(\"Within Stim Range\", fontsize=16)\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "# Second subplot (all data) - PARA SACAR LOS PUNTOS AZULES SOLO PONGO # ANTES DE AXS [0] PARA LOS BLUE\n",
    "#axs[1].scatter(concatenated_low_D[:, 0][concatenated_colors == \"blue\"], concatenated_low_D[:, 1][concatenated_colors == \"blue\"], color=\"blue\", alpha=0.3, s=10)\n",
    "axs[1].scatter(concatenated_low_D[:, 0][concatenated_colors == \"red\"], concatenated_low_D[:, 1][concatenated_colors == \"red\"], color=\"red\", alpha=0.6, s=40)\n",
    "axs[1].scatter(concatenated_low_D[:, 0][concatenated_colors == \"green\"], concatenated_low_D[:, 1][concatenated_colors == \"green\"], color=\"green\", alpha=0.6, s=40)\n",
    "axs[1].set_title(\"All Events\", fontsize=16)\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "#plt.savefig(f\"01 - 2D representation of escape events.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Contar puntos rojos y verdes en el primer subplot (within_frame_range)\n",
    "red_points_in_range = np.sum(concatenated_colors_in_range == \"red\")\n",
    "green_points_in_range = np.sum(concatenated_colors_in_range == \"green\")\n",
    "\n",
    "# Contar puntos rojos y verdes en el segundo subplot (all data)\n",
    "red_points_all = np.sum(concatenated_colors == \"red\")\n",
    "green_points_all = np.sum(concatenated_colors == \"green\")\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"First subplot (Within Stim Range):\")\n",
    "print(f\"Number of red points: {red_points_in_range}\")\n",
    "print(f\"Number of green points: {green_points_in_range}\\n\")\n",
    "\n",
    "print(f\"Second subplot (All Events):\")\n",
    "print(f\"Number of red points: {red_points_all}\")\n",
    "print(f\"Number of green points: {green_points_all}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_low_D.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e300743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CON ESTE CODIGO ME DA UN GRAFICO EN EL CUAL SELECCIONO LOS PUNTOS DE ARRIBA (NO ME PONE TODOS LOS EVENTOS - ES DECIR NO PONE LOS PUNTOS AZULES).\n",
    "\n",
    "%matplotlib notebook \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "import numpy as np\n",
    "\n",
    "# Filtrar los puntos que tienen colores rojos y verdes\n",
    "filtered_points = concatenated_low_D[(concatenated_colors == \"red\") | (concatenated_colors == \"green\")]\n",
    "\n",
    "# Inicializar la figura\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Mostrar solo los puntos filtrados (rojos y verdes)\n",
    "scatter_plot = ax.scatter(filtered_points[:, 0], filtered_points[:, 1], c='gray', s=5)\n",
    "\n",
    "coords = []\n",
    "polygons = []\n",
    "cluster_count = 0\n",
    "labels = np.full(filtered_points.shape[0], -1)  # Initialize labels to -1 (unassigned)\n",
    "\n",
    "def onclick(event):\n",
    "    global ix, iy, coords\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    print(f'x = {ix}, y = {iy}')\n",
    "\n",
    "    coords.append((ix, iy))\n",
    "\n",
    "    polygon = plt.Polygon(coords, fill=None, edgecolor='r')\n",
    "    ax.add_patch(polygon)\n",
    "\n",
    "def onkey(event):\n",
    "    global coords, polygons, cluster_count, labels\n",
    "    if event.key == 'enter':\n",
    "        print(\"Polygon complete. Checking points...\")\n",
    "        \n",
    "        path = Path(coords)\n",
    "        mask = path.contains_points(filtered_points)\n",
    "        \n",
    "        # Update labels\n",
    "        labels[mask] = cluster_count\n",
    "        \n",
    "        # Save the shape of the polygon\n",
    "        polygons.append(path)\n",
    "        \n",
    "        # Update the color map based on the cluster labels\n",
    "        scatter_plot.set_array(labels)\n",
    "        \n",
    "        # Increase the cluster count\n",
    "        cluster_count += 1\n",
    "        \n",
    "        coords = []  # Reset coords\n",
    "\n",
    "        plt.draw()\n",
    "\n",
    "# Conectar eventos para clicks y teclas\n",
    "cid_click = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "cid_key = fig.canvas.mpl_connect('key_press_event', onkey)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL - POR EL MOMENTO NO USARLO\n",
    "# con este codigo entero me da el plot con todos los eventos para seleccionar puntos que podrían ser cluster para mí.\n",
    "\n",
    "%matplotlib notebook \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "import numpy as np\n",
    "\n",
    "# Sample 2D data (replace this with your t-SNE or other low-dimensional data)\n",
    "points = concatenated_low_D\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "scatter_plot = ax.scatter(points[:, 0], points[:, 1], c='gray', s=5)\n",
    "\n",
    "coords = []\n",
    "polygons = []\n",
    "cluster_count = 0\n",
    "labels = np.full(points.shape[0], -1)  # Initialize labels to -1 (unassigned)\n",
    "\n",
    "def onclick(event):\n",
    "    global ix, iy, coords\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    print(f'x = {ix}, y = {iy}')\n",
    "\n",
    "    coords.append((ix, iy))\n",
    "\n",
    "    polygon = plt.Polygon(coords, fill=None, edgecolor='r')\n",
    "    ax.add_patch(polygon)\n",
    "\n",
    "def onkey(event):\n",
    "    global coords, polygons, cluster_count, labels\n",
    "    if event.key == 'enter':\n",
    "        print(\"Polygon complete. Checking points...\")\n",
    "        \n",
    "        path = Path(coords)\n",
    "        mask = path.contains_points(points)\n",
    "        \n",
    "        # Update labels\n",
    "        labels[mask] = cluster_count\n",
    "        \n",
    "        # Save the shape of the polygon\n",
    "        polygons.append(path)\n",
    "        \n",
    "        # Update the color map based on the cluster labels\n",
    "        scatter_plot.set_array(labels)\n",
    "        \n",
    "        # Increase the cluster count\n",
    "        cluster_count += 1\n",
    "        \n",
    "        coords = []  # Reset coords\n",
    "\n",
    "        plt.draw()\n",
    "\n",
    "cid_click = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "cid_key = fig.canvas.mpl_connect('key_press_event', onkey)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "# en este caso \"-1\" son todos los puntos que NO seleccioné,y \"0\" son los puntos que SI seleccioné.\n",
    "%matplotlib inline\n",
    "np.unique(labels)\n",
    "# np.sum(labels==0)   #si corro esta línea me debería dar la cantidad de puntos que seleccioné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 21: Asignación de event_selection con verificación\n",
    "index_counter = 0\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        num_events = len(trial['valid_event_times'])\n",
    "        \n",
    "        # Asegúrate de que no excedas el tamaño de labels\n",
    "        if index_counter + num_events <= len(labels):\n",
    "            trial['event_selection'] = labels[index_counter:index_counter + num_events]\n",
    "        else:\n",
    "            print(f\"Index error for stimulus {stimulus}: index_counter={index_counter}, num_events={num_events}, labels_length={len(labels)}\")\n",
    "        \n",
    "        index_counter += num_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "# Assuming 'labels' contains the cluster information for all events #ORIGINAL\n",
    "\n",
    "index_counter = 0\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        #within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1])\n",
    "        num_events = len(trial['valid_event_times'])\n",
    "        trial['event_selection'] = labels[index_counter:index_counter + num_events]\n",
    "        index_counter += num_events\n",
    "        \n",
    "print(index_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 22: Cálculo de max_velocities\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        trial['valid_event_ends'] = []\n",
    "        \n",
    "        for valid_event_time in trial['valid_event_times']:\n",
    "            index = trial['event_times'].index(valid_event_time)\n",
    "            trial['valid_event_ends'].append(trial['event_ends'][index])\n",
    "        \n",
    "        trial['valid_event_ends'] = np.array(trial['valid_event_ends'])\n",
    "        \n",
    "        trial['max_velocities'] = []\n",
    "        \n",
    "        for i in range(len(trial['valid_event_times'])):\n",
    "            if i < len(trial['event_selection']):  # Verifica que estés dentro de los límites\n",
    "                event_time = trial['valid_event_times'][i]\n",
    "                event_end = trial['valid_event_ends'][i]\n",
    "\n",
    "                velocities = trial['features']['sum_curvature'][event_time:event_end] # ACÁ SI QUIERO VER OTRAS VARIABLES SOLO CAMBIO LO QUE ESTÁ DESPUES DE \"features\", es decir donde dice head_velocity (velocities = trial['features']['head_velocity'])\n",
    "                trial['max_velocities'].append(velocities.max())\n",
    "        \n",
    "        trial['max_velocities'] = np.array(trial['max_velocities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "# for stimulus, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "#             continue\n",
    "        \n",
    "#         trial['valid_event_ends'] = []\n",
    "        \n",
    "#         for valid_event_time in trial['valid_event_times']:\n",
    "#             index = trial['event_times'].index(valid_event_time)\n",
    "#             trial['valid_event_ends'].append(trial['event_ends'][index])\n",
    "        \n",
    "#         trial['valid_event_ends'] = np.array(trial['valid_event_ends'])\n",
    "        \n",
    "#         trial['max_velocities'] = []\n",
    "        \n",
    "#         for i in range(len(trial['valid_event_times'])):\n",
    "#             event_time = trial['valid_event_times'][i]\n",
    "#             event_end = trial['valid_event_ends'][i]\n",
    "            \n",
    "#             velocities = trial['features']['angular_velocity'][event_time:event_end] #si quiero seleccionar otra variable en lugar de 'head_velocity' pongo otra variable como la velocidad angular etc. Lo que si tengo que cambiar tambien el lugar donde se guarda todo, en lugar de 'max_velocities' debería poner algo asi como 'max_velocidad_angular'.\n",
    "#             trial['max_velocities'].append(velocities.max())\n",
    "            \n",
    "        \n",
    "#         trial['max_velocities'] = np.array(trial['max_velocities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of max_velocities: {len(trial['max_velocities'])}\")\n",
    "print(f\"Number of event_selection: {len(trial['event_selection'])}\")\n",
    "print(f\"Total labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trial['event_selection']== selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ccce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trial['max_velocities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ec8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 23: Obtener la distribución de max_velocities\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "selected = 1  # Elige qué grupo de puntos seleccionado quieres plotear.\n",
    "all_max_velocities = []\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "            \n",
    "        # Asegúrate de que estés accediendo a los índices correctos\n",
    "        all_max_velocities.extend(trial['max_velocities'][trial['event_selection'] == selected])  # Filtrado\n",
    "        \n",
    "all_max_velocities = np.array(all_max_velocities)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(all_max_velocities[all_max_velocities < 20], bins=40)\n",
    "plt.xlabel('suma de la curvatura')  # Cambia según la variable que estés usando\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORIGINAL\n",
    "import matplotlib.pyplot as plt #con este codigo voy a obtener un histograma con la distribución de las velocidades o de la variable que quiero.\n",
    "\n",
    "selected = 0 #acá voy a elegir que grupo de puntos seleccionado quiero plotear. '0' es el primer grupo de puntos seleccionado, si quiero plotear el segundo pongo '1', el tercero es '2'.\n",
    "all_max_velocities = []\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        all_max_velocities.extend(trial['max_velocities'][trial['event_selection'] == selected]) #supuestamente acá está el error cuando quiero obtener la velocidad de los puntos en estimulos unisensoriales para sociales/aislados solo\n",
    "        \n",
    "all_max_velocities = np.array(all_max_velocities)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(all_max_velocities[all_max_velocities < 20], bins = 40)\n",
    "plt.xlabel('angular_acceleration') #lo cambié, el original decia 'Max Velocities for Selected Events'. Antes puse velocidad angular.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fce537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN, OPTICS\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import matplotlib.colors as mcolors\n",
    "\n",
    "# # # Initialize parameters for each algorithm\n",
    "# # n_clusters = 5 # Number of clusters for algorithms that require it\n",
    "\n",
    "# # # # DBSCAN parameters\n",
    "# # eps = 0.7\n",
    "# # min_samples = 7\n",
    "\n",
    "# eps = 5.5\n",
    "# min_samples = 25\n",
    "\n",
    "# # # # OPTICS parameters\n",
    "# # # min_samples_optics = 5\n",
    "# # # xi = 0.05\n",
    "# # # min_cluster_size = 0.05\n",
    "\n",
    "# # # Uncomment one of the following clustering algorithms to use\n",
    "\n",
    "# # # K-Means\n",
    "# # #labels = KMeans(n_clusters=n_clusters).fit_predict(concatenated_low_D)\n",
    "\n",
    "# # # Agglomerative Clustering\n",
    "# # #labels = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(concatenated_low_D)\n",
    "\n",
    "# # # Spectral Clustering\n",
    "# # labels = SpectralClustering(n_clusters=n_clusters).fit_predict(concatenated_low_D)\n",
    "\n",
    "# # # Gaussian Mixture Model\n",
    "# # #gmm = GaussianMixture(n_components=n_clusters).fit(concatenated_low_D)\n",
    "# # #labels = gmm.predict(concatenated_low_D)\n",
    "\n",
    "# # # DBSCAN\n",
    "# dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "# labels = dbscan.fit_predict(concatenated_low_D)\n",
    "\n",
    "# # # OPTICS\n",
    "# # #optics = OPTICS(min_samples=min_samples_optics, xi=xi, min_cluster_size=min_cluster_size)\n",
    "# # #labels = optics.fit_predict(concatenated_low_D)\n",
    "\n",
    "# # # Add cluster labels to each trial's 'is_cluster' field\n",
    "# index_counter = 0\n",
    "# for stimulus, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1])\n",
    "#         num_events = len(trial['is_escape'][within_frame_range])\n",
    "#         trial['is_cluster'] = labels[index_counter:index_counter + num_events]\n",
    "#         index_counter += num_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of distinct colors\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(np.unique(labels))))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "# Scatter plot\n",
    "for i, label in enumerate(np.unique(labels)):\n",
    "    cluster_data = concatenated_low_D[labels == label]\n",
    "    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], color=colors[i % len(colors)], label=label, alpha=1, s=15)\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig(f\"02 - 2D representation of events, clustered by hand.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'labels' contains the cluster information for all events\n",
    "\n",
    "index_counter = 0\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        #within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1])\n",
    "        num_events = len(trial['valid_event_times'])\n",
    "        trial['event_clusters'] = labels[index_counter:index_counter + num_events]\n",
    "        index_counter += num_events\n",
    "        \n",
    "print(index_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fee52c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# Get a list of distinct colors\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(np.unique(labels))))\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1])\n",
    "        clusters = np.array(trial['event_clusters'][within_frame_range])\n",
    "        points = trial['low_D_representation'][within_frame_range]\n",
    "        \n",
    "        plt.scatter(points[:, 0], points[:, 1], color=colors[clusters+1], alpha=1, s=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ea473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of distinct colors\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(np.unique(labels))))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "# Scatter plot\n",
    "for i, label in enumerate(np.unique(labels)):\n",
    "    cluster_data = concatenated_low_D[labels == label]\n",
    "    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], color=colors[i % len(colors)], label=label, alpha=1, s=15)\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig(f\"02 - 2D representation of events, clustered by hand.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6728bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric to classic code mapping\n",
    "numeric_to_classic = {\n",
    "    '0.0': 'no',  # Represents no stimulus\n",
    "    '0.1': 'min',\n",
    "    '0.3': 'med',\n",
    "    '1.0': 'max'\n",
    "}\n",
    "\n",
    "# Visual and auditory classic codes\n",
    "visual_codes = {value: 'v' + value for key, value in numeric_to_classic.items()}\n",
    "auditory_codes = {value: 'a' + value for key, value in numeric_to_classic.items()}\n",
    "\n",
    "# Combining visual and auditory codes for multisensory stimuli\n",
    "stim_pairs_numeric = sorted(trials.keys())\n",
    "\n",
    "conversion_dict = {}\n",
    "\n",
    "for pair in stim_pairs_numeric:\n",
    "    v_intensity, a_intensity = pair.split('_')\n",
    "    v_code = visual_codes[numeric_to_classic[v_intensity]]\n",
    "    a_code = auditory_codes[numeric_to_classic[a_intensity]]\n",
    "    # For no stimulus case, use \"ctrl\" instead of combining \"no\" labels\n",
    "    if v_intensity == '0.0' and a_intensity == '0.0':\n",
    "        conversion_dict[pair] = 'ctrl'\n",
    "    else:\n",
    "        conversion_dict[pair] = f\"{v_code}_{a_code}\" if v_intensity != '0.0' and a_intensity != '0.0' else v_code if a_intensity == '0.0' else a_code\n",
    "\n",
    "print(conversion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26911097",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 50\n",
    "\n",
    "print(trials[\"1.0_1.0\"][t].keys())\n",
    "print(trials[\"1.0_1.0\"][t][\"metadata\"].keys())\n",
    "print(trials[\"1.0_1.0\"][t][\"metadata\"][\"F_stim\"])\n",
    "print(trials[\"1.0_1.0\"][t][\"features\"].keys())\n",
    "print(trials[\"1.0_1.0\"][t][\"event_times\"])\n",
    "print(trials[\"1.0_1.0\"][t][\"valid_event_times\"])\n",
    "print(trials[\"1.0_1.0\"][t][\"features\"]['head_velocity'].shape)\n",
    "print(trials[\"1.0_1.0\"][t][\"features\"]['angular_acceleration'].shape)\n",
    "print(trials[\"1.0_1.0\"][t][\"event_features\"].shape)\n",
    "print(trials[\"1.0_1.0\"][t][\"low_D_representation\"].shape)\n",
    "print(trials[\"1.0_1.0\"][t][\"event_clusters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f5ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_F_stims = []\n",
    "\n",
    "for stim in trials:\n",
    "    for trial in trials[stim]:\n",
    "        all_F_stims.append(trial[\"metadata\"][\"F_stim\"])\n",
    "        \n",
    "plt.hist(all_F_stims)\n",
    "plt.show()\n",
    "\n",
    "print(min(all_F_stims), max(all_F_stims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed1db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "organized_data = {}\n",
    "\n",
    "relative_stim_window = (-500,500)\n",
    "\n",
    "frames_before = 8000\n",
    "frames_after = 4000\n",
    "\n",
    "for intensity, stim_code in conversion_dict.items():\n",
    "    print(intensity, stim_code)\n",
    "    organized_data[stim_code] = {\"trials\": [], \"videos\": [], \"fish\": [], \"sequence_in_fish\": [], \"sequence_in_day\": [], \n",
    "                                \"sequence_in_exp\": [], \"age\": [], \"vis_intensity\": [], \"aud_intensity\": [], \n",
    "                                \"head_velocity\": [], \"tail_velocity\": [], \"angular_velocity\": [], \"sum_tail_angles\": [],\n",
    "                                \"tail6_velocity\": [], \"head_tail_distance\": [], \"head_acceleration\": [], \"tail_acceleration\": [], \n",
    "                                \"angular_acceleration\": [], \"head_jerk\": [], \"tail_jerk\": [], \"sum_curvature\": [],\n",
    "                                \"angle_velocity_heading\": [], \"curvature_rate\": [], \"event_times\": [], \"valid_event_times\": [],\n",
    "                                \"events_in_high_dim\": [], \"events_in_low_dim\": [], \"behavior_type\": [], \"is_escape\": []}\n",
    "    \n",
    "    for trial in trials[intensity]:\n",
    "        F_stim = int(trial[\"metadata\"][\"F_stim\"])\n",
    "        \n",
    "        organized_data[stim_code][\"trials\"].append(int(trial[\"metadata\"][\"Trial\"]))\n",
    "        organized_data[stim_code][\"videos\"].append(trial[\"metadata\"][\"Video\"])\n",
    "        organized_data[stim_code][\"fish\"].append(int(trial[\"metadata\"][\"Pez\"]))\n",
    "        organized_data[stim_code][\"sequence_in_fish\"].append(int(trial[\"metadata\"][\"Secuencia\"]))\n",
    "        organized_data[stim_code][\"sequence_in_day\"].append(int(trial[\"metadata\"][\"Sec_día\"]))\n",
    "        organized_data[stim_code][\"sequence_in_exp\"].append(int(trial[\"metadata\"][\"Sec_Exp\"]))\n",
    "        organized_data[stim_code][\"age\"].append(int(trial[\"metadata\"][\"Edad_días\"]))\n",
    "        organized_data[stim_code][\"vis_intensity\"].append(float(trial[\"metadata\"][\"Visual\"]))\n",
    "        organized_data[stim_code][\"aud_intensity\"].append(float(trial[\"metadata\"][\"Auditivo\"]))\n",
    "        \n",
    "        organized_data[stim_code][\"head_velocity\"].append(trial[\"features\"]['head_velocity'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"tail_velocity\"].append(trial[\"features\"]['tail_velocity'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"angular_velocity\"].append(trial[\"features\"]['angular_velocity'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"sum_tail_angles\"].append(trial[\"features\"]['sum_tail_angles'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"tail6_velocity\"].append(trial[\"features\"]['tail6_velocity'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"head_tail_distance\"].append(trial[\"features\"]['head_tail_distance'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"head_acceleration\"].append(trial[\"features\"]['head_acceleration'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"tail_acceleration\"].append(trial[\"features\"]['tail_acceleration'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"angular_acceleration\"].append(trial[\"features\"]['angular_acceleration'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"head_jerk\"].append(trial[\"features\"]['head_jerk'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"tail_jerk\"].append(trial[\"features\"]['tail_jerk'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"sum_curvature\"].append(trial[\"features\"]['sum_curvature'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"angle_velocity_heading\"].append(trial[\"features\"]['angle_velocity_heading'][F_stim-frames_before:F_stim+frames_after])\n",
    "        organized_data[stim_code][\"curvature_rate\"].append(trial[\"features\"]['curvature_rate'][F_stim-frames_before:F_stim+frames_after])\n",
    "        \n",
    "        event_times = np.array(trial[\"event_times\"])\n",
    "        event_times = event_times[(event_times > (F_stim-frames_before)) & (event_times < (F_stim+frames_after))]\n",
    "        event_times = event_times - (F_stim-frames_before)\n",
    "        organized_data[stim_code][\"event_times\"].append(event_times)\n",
    "        \n",
    "        valid_event_times = np.array(trial[\"valid_event_times\"])\n",
    "        valid_event_times = valid_event_times[(valid_event_times > (F_stim-frames_before)) & (valid_event_times < (F_stim+frames_after))]\n",
    "        valid_event_times = valid_event_times - (F_stim-frames_before)\n",
    "        organized_data[stim_code][\"valid_event_times\"].append(valid_event_times)\n",
    "        \n",
    "        valid_event_times = np.array(trial[\"valid_event_times\"])\n",
    "        organized_data[stim_code][\"events_in_high_dim\"].append(trial[\"event_features\"][(valid_event_times > (F_stim-frames_before)) & (valid_event_times < (F_stim+frames_after))])\n",
    "        \n",
    "        if \"low_D_representation\" in trial: \n",
    "            organized_data[stim_code][\"events_in_low_dim\"].append(trial[\"low_D_representation\"][(valid_event_times > (F_stim-frames_before)) & (valid_event_times < (F_stim+frames_after))])\n",
    "            organized_data[stim_code][\"behavior_type\"].append(np.array(trial[\"event_clusters\"][(valid_event_times > (F_stim-frames_before)) & (valid_event_times < (F_stim+frames_after))]))\n",
    "        else:\n",
    "            organized_data[stim_code][\"events_in_low_dim\"].append(np.zeros((0,2)))\n",
    "            organized_data[stim_code][\"behavior_type\"].append(np.zeros((0,)))\n",
    "        \n",
    "        valid_event_times = np.array(trial[\"valid_event_times\"])\n",
    "        valid_event_times = valid_event_times[(valid_event_times > (F_stim+relative_stim_window[0])) & (valid_event_times < (F_stim+relative_stim_window[1]))]\n",
    "        organized_data[stim_code][\"is_escape\"].append(len(valid_event_times) > 0)\n",
    "    \n",
    "    for key in organized_data[stim_code].keys():\n",
    "        if key not in [\"videos\", \"event_times\", \"valid_event_times\", \"events_in_high_dim\", \"events_in_low_dim\", \"behavior_type\"]:\n",
    "            organized_data[stim_code][key] = np.array(organized_data[stim_code][key])\n",
    "            print(key, organized_data[stim_code][key].shape)\n",
    "        else:\n",
    "            print(key, len(organized_data[stim_code][key]))\n",
    "            \n",
    "    print()\n",
    "\n",
    "organized_data[\"stim_time\"] = frames_before\n",
    "organized_data[\"stim_window\"] = (frames_before+relative_stim_window[0], frames_before+relative_stim_window[1])\n",
    "\n",
    "organized_data[\"colors\"] = {\"ctrl\":(0.5,0.5,0.5),\n",
    "                            \"vmin\":(1,0.8,0.8),\n",
    "                            \"vmed\":(1,0.4,0.4),\n",
    "                            \"vmax\":(1,0,0),\n",
    "                            \"amin\":(0.8,0.8,1),\n",
    "                            \"amed\":(0.4,0.4,1),\n",
    "                            \"amax\":(0,0,1),\n",
    "                            \"vmin_amin\":(1,0.8,1),\n",
    "                            \"vmed_amin\":(1,0.5,0.85),\n",
    "                            \"vmax_amin\":(1,0.3,0.7),\n",
    "                            \"vmin_amed\":(0.8,0.6,1),\n",
    "                            \"vmed_amed\":(0.9,0.375,0.925),\n",
    "                            \"vmax_amed\":(1,0.15,0.85),\n",
    "                            \"vmin_amax\":(0.6,0.4,1),\n",
    "                            \"vmed_amax\":(0.8,0.2,1),\n",
    "                            \"vmax_amax\":(1,0,1)}\n",
    "\n",
    "# Create the stim_codes dictionary\n",
    "organized_data[\"stim_labels\"] = {\"ctrl\":\"Control\",\n",
    "                                \"vmin\":\"Vis Min\",\n",
    "                                \"vmed\":\"Vis Med\",\n",
    "                                \"vmax\":\"Vis Max\",\n",
    "                                \"amin\":\"Aud Min\",\n",
    "                                \"amed\":\"Aud Med\",\n",
    "                                \"amax\":\"Aud Max\",\n",
    "                                \"vmin_amin\":\"Vis Min + Aud Min\",\n",
    "                                \"vmed_amin\":\"Vis Med + Aud Min\",\n",
    "                                \"vmax_amin\":\"Vis Max + Aud Min\",\n",
    "                                \"vmin_amed\":\"Vis Min + Aud Med\",\n",
    "                                \"vmed_amed\":\"Vis Med + Aud Med\",\n",
    "                                \"vmax_amed\":\"Vis Max + Aud Med\",\n",
    "                                \"vmin_amax\":\"Vis Min + Aud Max\",\n",
    "                                \"vmed_amax\":\"Vis Min + Aud Max\",\n",
    "                                \"vmax_amax\":\"Vis Max + Aud Max\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = organized_data\n",
    "i = 5\n",
    "for stim in data:\n",
    "    if \"max\" in stim or \"med\" in stim or \"min\" in stim:\n",
    "        print(stim, len(data[stim][\"valid_event_times\"]), data[stim][\"valid_event_times\"][i].shape, len(data[stim][\"behavior_type\"]), data[stim][\"behavior_type\"][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "os.chdir(r\"D:\\Videos Zebrafish\\ConfocalLike3 (Febrero 2024)\")\n",
    "\n",
    "# Writing the list to a file using Pickle\n",
    "with open('organized_data_behavior_2024_new_tracking.pkl', 'wb') as file:\n",
    "    pickle.dump(organized_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5046a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def show_color(rgb):\n",
    "    \"\"\"\n",
    "    Displays a color in a big square based on RGB values.\n",
    "\n",
    "    Parameters:\n",
    "    rgb (tuple): A tuple of three numbers between 0 and 1, representing the RGB values.\n",
    "    \"\"\"\n",
    "    # Create a figure and a subplot\n",
    "    fig, ax = plt.subplots()\n",
    "    # Create a rectangle patch with the given RGB color\n",
    "    rect = patches.Rectangle((0.1, 0.1), 1.0, 1.0, linewidth=1, edgecolor='black', facecolor=rgb)\n",
    "    # Add the rectangle patch to the subplot\n",
    "    ax.add_patch(rect)\n",
    "    # Set the limits of the axes\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')  # Turn off the axis\n",
    "    plt.show()  # Display the figure\n",
    "\n",
    "colors = {\"ctrl\":(0.5,0.5,0.5),\n",
    "            \"vmin\":(1,0.8,0.8),\n",
    "            \"vmed\":(1,0.4,0.4),\n",
    "            \"vmax\":(1,0,0),\n",
    "            \"amin\":(0.8,0.8,1),\n",
    "            \"amed\":(0.4,0.4,1),\n",
    "            \"amax\":(0,0,1),\n",
    "            \"vmin_amin\":(1,0.8,1),\n",
    "            \"vmed_amin\":(1,0.5,0.85),\n",
    "            \"vmax_amin\":(1,0.3,0.7),\n",
    "            \"vmin_amed\":(0.8,0.6,1),\n",
    "            \"vmed_amed\":(0.9,0.375,0.925),\n",
    "            \"vmax_amed\":(1,0.15,0.85),\n",
    "            \"vmin_amax\":(0.6,0.4,1),\n",
    "            \"vmed_amax\":(0.8,0.2,1),\n",
    "            \"vmax_amax\":(1,0,1)}\n",
    "\n",
    "for key in colors:\n",
    "    show_color(colors[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming organized_data is your main data structure\n",
    "\n",
    "# Step 1: Calculate response probability per stim type and age group\n",
    "response_probabilities = {}  # {stim_type: {age: probability, ...}, ...}\n",
    "\n",
    "stim_types = ['ctrl', 'amin', 'amed', 'amax', 'vmin', 'vmin_amin', 'vmin_amed', 'vmin_amax', 'vmed', 'vmed_amin', 'vmed_amed', 'vmed_amax', 'vmax', 'vmax_amin', 'vmax_amed', 'vmax_amax']\n",
    "\n",
    "for stim_type in stim_types:\n",
    "    # Extract age and is_escape data\n",
    "    ages = organized_data[stim_type][\"age\"]\n",
    "    escapes = organized_data[stim_type][\"is_escape\"]\n",
    "    \n",
    "    # Group by age and calculate response probability\n",
    "    age_groups = np.unique(ages)\n",
    "    stim_response_probabilities = {}\n",
    "    \n",
    "    for age in age_groups:\n",
    "        is_escape_at_age = escapes[ages == age]\n",
    "        probability = np.mean(is_escape_at_age)\n",
    "        stim_response_probabilities[age] = probability\n",
    "    \n",
    "    response_probabilities[stim_type] = stim_response_probabilities\n",
    "\n",
    "# Step 2: Prepare the data for plotting - done during plotting to keep the structure simple\n",
    "\n",
    "# Step 3: Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for stim_type, age_probs in response_probabilities.items():\n",
    "    ages = list(age_probs.keys())\n",
    "    probabilities = list(age_probs.values())\n",
    "    plt.plot(ages, probabilities, label=organized_data[\"stim_labels\"][stim_type], color=organized_data[\"colors\"][stim_type])\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Response Probability')\n",
    "plt.title('Response Probability per Stim Type and Age')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# cluster_events = {}\n",
    "\n",
    "# for cluster in np.unique(labels):\n",
    "#     cluster_events[cluster] = []\n",
    "\n",
    "# for stimulus, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         for i, cluster in enumerate(trial['event_clusters']):\n",
    "#             event_time = trial['valid_event_times'][i]\n",
    "#             cluster_events[cluster].append((trial['video_name'], event_time))\n",
    "\n",
    "# # Randomly sample 10 events from each cluster\n",
    "# for cluster, events in cluster_events.items():\n",
    "#     cluster_events[cluster] = random.sample(events, min(10, len(events)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0455b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "cluster_events = {}\n",
    "\n",
    "for cluster in np.unique(labels):\n",
    "    cluster_events[cluster] = []\n",
    "\n",
    "for stimulus, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        for i, cluster in enumerate(trial['event_clusters']):\n",
    "            event_time = trial['valid_event_times'][i]\n",
    "            cluster_events[cluster].append((trial['video_name'], event_time, trial['event_features_over_time'][i]))\n",
    "\n",
    "# Randomly sample 10 events from each cluster except for cluster 1, from which we'll sample 30 events\n",
    "for cluster, events in cluster_events.items():\n",
    "    if cluster == -1:\n",
    "        cluster_events[cluster] = random.sample(events, min(1, len(events)))\n",
    "    else:\n",
    "        cluster_events[cluster] = random.sample(events, min(10, len(events)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# all_low_D_representations = []\n",
    "# all_is_escape = []\n",
    "\n",
    "# # Assume `trials` is your dictionary containing all the data\n",
    "# for stimulus_type, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "#             continue\n",
    "        \n",
    "#         all_low_D_representations.append(trial['low_D_representation'])\n",
    "#         all_is_escape.append(trial['is_escape'])\n",
    "\n",
    "# all_low_D_representations = np.vstack(all_low_D_representations)\n",
    "# all_is_escape = np.concatenate(all_is_escape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# # Prepare the data\n",
    "# escape_points = all_low_D_representations[all_is_escape]\n",
    "# non_escape_points = all_low_D_representations[~all_is_escape]\n",
    "\n",
    "# # Perform KDE\n",
    "# kde_escape = KernelDensity(kernel='gaussian', bandwidth=0.35).fit(escape_points)\n",
    "# kde_non_escape = KernelDensity(kernel='gaussian', bandwidth=0.35).fit(non_escape_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb96da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a 2D grid over which to evaluate the KDE\n",
    "# x_vals = np.linspace(min(all_low_D_representations[:, 0]), max(all_low_D_representations[:, 0]), 100)\n",
    "# y_vals = np.linspace(min(all_low_D_representations[:, 1]), max(all_low_D_representations[:, 1]), 100)\n",
    "# xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "# grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# # Evaluate KDE at grid points\n",
    "# pdf_escape = np.exp(kde_escape.score_samples(grid_points))\n",
    "# pdf_non_escape = np.exp(kde_non_escape.score_samples(grid_points))\n",
    "\n",
    "# # Calculate ratio\n",
    "# pdf_ratio = pdf_escape / (pdf_escape + pdf_non_escape + 1e-10)  # Adding a small constant to avoid division by zero\n",
    "# pdf_ratio = pdf_ratio.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = go.Figure(data=[go.Surface(z=pdf_ratio, x=xx, y=yy)])\n",
    "# fig.update_layout(scene=dict(zaxis=dict(range=[0, np.max(pdf_ratio)])))\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.interpolate import interp2d\n",
    "\n",
    "# threshold = 0.35\n",
    "\n",
    "# above_threshold = pdf_ratio > threshold\n",
    "\n",
    "# # Create an interpolation function based on the calculated ratio\n",
    "# interp_func = interp2d(x_vals, y_vals, pdf_ratio, kind='linear')\n",
    "\n",
    "# # Evaluate the function at the points in all_low_D_representations\n",
    "# evaluated_ratios = np.array([interp_func(x, y)[0] for x, y in all_low_D_representations])\n",
    "\n",
    "# # Create estimated_escape array based on the threshold\n",
    "# estimated_escape = (evaluated_ratios > threshold).astype(int)\n",
    "\n",
    "# print(estimated_escape.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.scatter(all_low_D_representations[:, 0], all_low_D_representations[:, 1], c=estimated_escape, cmap='viridis', s=10)\n",
    "# plt.colorbar(label='Estimated Escape')\n",
    "# plt.xlabel('Dimension 1')\n",
    "# plt.ylabel('Dimension 2')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_start = 0\n",
    "\n",
    "# for stimulus_type, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "#             continue\n",
    "            \n",
    "#         num_events = trial['low_D_representation'].shape[0]\n",
    "#         trial['estimated_escape'] = estimated_escape[index_start:index_start + num_events]\n",
    "#         trial['estimated_escape'] = trial['estimated_escape'].astype('bool')\n",
    "#         index_start += num_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_start = 0\n",
    "\n",
    "# for stimulus_type, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "#             continue\n",
    "            \n",
    "#         clust = trial['is_cluster']\n",
    "        \n",
    "#         fast_escape = np.array([clust == 0, clust == 1, clust == 5, clust == 6, clust == 9]).any(axis=0)\n",
    "        \n",
    "#         trial['estimated_escape'] = fast_escape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "\n",
    "# Initialize lists to store the values for each parameter\n",
    "params_cluster_0 = {\n",
    "    'max_head_velocity': [],\n",
    "    'max_tail_velocity': [],\n",
    "    'max_abs_angular_velocity': [],\n",
    "    'min_head_tail_distance': [],\n",
    "    'max_abs_angular_acceleration': [],\n",
    "    'max_abs_curvature': [],\n",
    "    'max_abs_curvature_rate': [],\n",
    "    'max_abs_head_jerk': []\n",
    "}\n",
    "\n",
    "params_other_clusters = {\n",
    "    'max_head_velocity': [],\n",
    "    'max_tail_velocity': [],\n",
    "    'max_abs_angular_velocity': [],\n",
    "    'min_head_tail_distance': [],\n",
    "    'max_abs_angular_acceleration': [],\n",
    "    'max_abs_curvature': [],\n",
    "    'max_abs_curvature_rate': [],\n",
    "    'max_abs_head_jerk': []\n",
    "}\n",
    "\n",
    "bonferroni_alpha = 0.05 / 8\n",
    "\n",
    "# Iterate over each trial and its events\n",
    "for stim_type, trials_i in trials.items():\n",
    "    for trial in trials_i:\n",
    "        event_clusters = trial['event_clusters']\n",
    "        valid_event_times = trial['valid_event_times']\n",
    "        features = trial['features']\n",
    "\n",
    "        for idx, event_time in enumerate(valid_event_times):\n",
    "            cluster = event_clusters[idx]\n",
    "\n",
    "            # Determine the event window\n",
    "            start_frame = event_time - 5\n",
    "            end_frame = event_time + 25\n",
    "\n",
    "            # Extract the relevant features within the event window\n",
    "            head_velocity = features['head_velocity'][start_frame:end_frame]\n",
    "            tail_velocity = features['tail_velocity'][start_frame:end_frame]\n",
    "            angular_velocity = features['angular_velocity'][start_frame:end_frame]\n",
    "            head_tail_distance = features['head_tail_distance'][start_frame:end_frame]\n",
    "            angular_acceleration = features['angular_acceleration'][start_frame:end_frame]\n",
    "            curvature = features['sum_curvature'][start_frame:end_frame]\n",
    "            curvature_rate = features['curvature_rate'][start_frame:end_frame]\n",
    "            head_jerk = features['head_jerk'][start_frame:end_frame]\n",
    "\n",
    "            # Compute the representative parameters for the event\n",
    "            params = {\n",
    "                'max_head_velocity': np.max(head_velocity),\n",
    "                'max_tail_velocity': np.max(tail_velocity),\n",
    "                'max_abs_angular_velocity': np.max(np.abs(angular_velocity)),\n",
    "                'min_head_tail_distance': np.min(head_tail_distance),\n",
    "                'max_abs_angular_acceleration': np.max(np.abs(angular_acceleration)),\n",
    "                'max_abs_curvature': np.max(np.abs(curvature)),\n",
    "                'max_abs_curvature_rate': np.max(np.abs(curvature_rate)),\n",
    "                'max_abs_head_jerk': np.max(np.abs(head_jerk))\n",
    "            }\n",
    "\n",
    "            # Store the parameters in the appropriate list\n",
    "            if cluster == 0:\n",
    "                for key in params.keys():\n",
    "                    params_cluster_0[key].append(params[key])\n",
    "            else:\n",
    "                for key in params.keys():\n",
    "                    params_other_clusters[key].append(params[key])\n",
    "\n",
    "# Now plot the mirrored histograms\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "fig, axes = plt.subplots(4, 2)\n",
    "\n",
    "for idx, param_name in enumerate(params_cluster_0.keys()):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Calculate min and max values for both cluster 0 and other clusters\n",
    "    min_value = min(np.percentile(params_cluster_0[param_name],5), np.percentile(params_other_clusters[param_name],5))\n",
    "    max_value = max(np.percentile(params_cluster_0[param_name],95), np.percentile(params_other_clusters[param_name],95))\n",
    "    \n",
    "    n_bins = 15\n",
    "    \n",
    "    # Define the bin edges\n",
    "    bin_edges = np.linspace(min_value, max_value, n_bins+1)\n",
    "    \n",
    "    # Plot histogram for cluster 0\n",
    "    sns.histplot(x=params_cluster_0[param_name], stat=\"density\", bins=bin_edges, edgecolor='black', ax=ax)\n",
    "    \n",
    "    # Plot histogram for other clusters\n",
    "    heights, bins = np.histogram(params_other_clusters[param_name], density=True, bins=bin_edges)\n",
    "    heights *= -1  # Multiply by -1 to reverse\n",
    "    bin_width = np.diff(bins)[0]\n",
    "    bin_pos = bins[:-1] + bin_width / 2  # Keep these positive\n",
    "    ax.bar(bin_pos, heights, width=bin_width, edgecolor='black')\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    stat, p_value = mannwhitneyu(params_cluster_0[param_name], params_other_clusters[param_name], alternative='two-sided')\n",
    "    \n",
    "    # Add the p-value to the plot\n",
    "    ax.text(0.1, 0.9, f'p = {p_value:.3f}', transform=ax.transAxes)\n",
    "    \n",
    "    if p_value < bonferroni_alpha:\n",
    "        ax.text(0.2, 0.75, '*', transform=ax.transAxes, fontsize=20)\n",
    "    \n",
    "    ax.set_title(param_name)\n",
    "\n",
    "#plt.savefig(f\"03 - Difference in feature distribution between C-starts and other escape events.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stim_window = (10000,11000)\n",
    "#stim_window = (9500,11500)\n",
    "\n",
    "desired_cluster = 1\n",
    "\n",
    "# Iterate through the trials to set 'is_estimated_escape'\n",
    "index_start = 0\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1]) & (trial['event_clusters'] == desired_cluster)\n",
    "        #trial_estimated_escapes = trial['estimated_escape'][within_frame_range]\n",
    "        \n",
    "        #trial['is_estimated_escape'] = any(trial_estimated_escapes)\n",
    "        event_clusters = trial['event_clusters']\n",
    "        trial['is_estimated_escape'] = within_frame_range.sum() > 0\n",
    "\n",
    "# Initialize a dictionary to hold escape probabilities for each stimulus type\n",
    "escape_probabilities = {}\n",
    "\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    is_estimated_escape_values = []\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        is_estimated_escape_values.append(trial['is_estimated_escape'])\n",
    "    escape_probabilities[stimulus_type] = np.mean(is_estimated_escape_values)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "visual_intensities = []\n",
    "auditory_intensities = []\n",
    "probabilities = []\n",
    "\n",
    "for stimulus_type, probability in escape_probabilities.items():\n",
    "    visual_intensity, auditory_intensity = map(float, stimulus_type.split('_'))\n",
    "    visual_intensities.append(visual_intensity)\n",
    "    auditory_intensities.append(auditory_intensity)\n",
    "    probabilities.append(probability)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Visual': visual_intensities,\n",
    "    'Auditory': auditory_intensities,\n",
    "    'Probability': probabilities\n",
    "})\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.pivot(\"Visual\", \"Auditory\", \"Probability\"), cmap=\"Oranges\", annot=True, cbar_kws={'label': 'Escape Probability'}, vmin=0, vmax=1)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Escape Probability Heatmap\")\n",
    "#plt.savefig(f\"04 - Observed probability matrix.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Function to calculate expected probability\n",
    "def calc_expected_p(p_v, p_a):\n",
    "    return p_v + p_a - p_v * p_a\n",
    "\n",
    "# Function to calculate integration coefficient\n",
    "def calc_integration(p_observed, p_expected):\n",
    "    return (p_observed - p_expected) / (p_observed + p_expected)\n",
    "\n",
    "# Initialize an empty DataFrame for storing integration coefficients\n",
    "integration_df = pd.DataFrame(columns=['Visual', 'Auditory', 'Integration'])\n",
    "\n",
    "# Iterate through each multisensory stimulus\n",
    "for _, row in df[(df['Visual'] != 0) & (df['Auditory'] != 0)].iterrows():\n",
    "    visual_intensity = row['Visual']\n",
    "    auditory_intensity = row['Auditory']\n",
    "    p_observed = row['Probability']\n",
    "    \n",
    "    # Get the probabilities of the associated unisensory stimuli\n",
    "    p_v = df[(df['Visual'] == visual_intensity) & (df['Auditory'] == 0)]['Probability'].values[0]\n",
    "    p_a = df[(df['Visual'] == 0) & (df['Auditory'] == auditory_intensity)]['Probability'].values[0]\n",
    "    \n",
    "    # Calculate expected probability\n",
    "    p_expected = calc_expected_p(p_v, p_a)\n",
    "    \n",
    "    # Calculate integration coefficient\n",
    "    integration = calc_integration(p_observed, p_expected)\n",
    "    \n",
    "    # Append to the DataFrame\n",
    "    integration_df = integration_df.append({'Visual': visual_intensity, 'Auditory': auditory_intensity, 'Integration': integration}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Pivot the DataFrame to get a matrix form suitable for heatmap\n",
    "integration_matrix = integration_df.pivot(\"Visual\", \"Auditory\", \"Integration\")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "heatmap = sns.heatmap(integration_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "heatmap.set_title('Integration Coefficients Heatmap')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis\n",
    "plt.show()\n",
    "\n",
    "# Initialize an empty DataFrame for storing integration coefficients\n",
    "integration_df = pd.DataFrame(columns=['Visual', 'Auditory', 'Integration'])\n",
    "\n",
    "# Iterate through each multisensory stimulus\n",
    "for _, row in df[(df['Visual'] != 0) & (df['Auditory'] != 0)].iterrows():\n",
    "    visual_intensity = row['Visual']\n",
    "    auditory_intensity = row['Auditory']\n",
    "    p_observed = row['Probability']\n",
    "    \n",
    "    # Get the probabilities of the associated unisensory stimuli\n",
    "    p_v = df[(df['Visual'] == visual_intensity) & (df['Auditory'] == 0)]['Probability'].values[0]\n",
    "    p_a = df[(df['Visual'] == 0) & (df['Auditory'] == auditory_intensity)]['Probability'].values[0]\n",
    "    \n",
    "    # Calculate expected probability\n",
    "    p_expected = max(p_v, p_a)\n",
    "    \n",
    "    # Calculate integration coefficient\n",
    "    integration = calc_integration(p_observed, p_expected)\n",
    "    \n",
    "    # Append to the DataFrame\n",
    "    integration_df = integration_df.append({'Visual': visual_intensity, 'Auditory': auditory_intensity, 'Integration': integration}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Pivot the DataFrame to get a matrix form suitable for heatmap\n",
    "integration_matrix = integration_df.pivot(\"Visual\", \"Auditory\", \"Integration\")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "heatmap = sns.heatmap(integration_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "heatmap.set_title('Integration Coefficients Heatmap')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cluster = 0\n",
    "\n",
    "# Iterate through the trials to set 'is_estimated_escape'\n",
    "index_start = 0\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1]) & (trial['event_clusters'] == desired_cluster)\n",
    "        #trial_estimated_escapes = trial['estimated_escape'][within_frame_range]\n",
    "        \n",
    "        #trial['is_estimated_escape'] = any(trial_estimated_escapes)\n",
    "        event_clusters = trial['event_clusters']\n",
    "        trial['is_estimated_escape'] = within_frame_range.sum() > 0\n",
    "\n",
    "# Initialize a dictionary to hold escape probabilities for each stimulus type\n",
    "escape_probabilities = {}\n",
    "\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    is_estimated_escape_values = []\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        is_estimated_escape_values.append(trial['is_estimated_escape'])\n",
    "    escape_probabilities[stimulus_type] = np.mean(is_estimated_escape_values)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "visual_intensities = []\n",
    "auditory_intensities = []\n",
    "probabilities = []\n",
    "\n",
    "for stimulus_type, probability in escape_probabilities.items():\n",
    "    visual_intensity, auditory_intensity = map(float, stimulus_type.split('_'))\n",
    "    visual_intensities.append(visual_intensity)\n",
    "    auditory_intensities.append(auditory_intensity)\n",
    "    probabilities.append(probability)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Visual': visual_intensities,\n",
    "    'Auditory': auditory_intensities,\n",
    "    'Probability': probabilities\n",
    "})\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.pivot(\"Visual\", \"Auditory\", \"Probability\"), cmap=\"Oranges\", annot=True, cbar_kws={'label': 'Escape Probability'}, vmin=0, vmax=1)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Escape Probability Heatmap\")\n",
    "#plt.savefig(f\"04 - Observed probability matrix.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Function to calculate expected probability\n",
    "def calc_expected_p(p_v, p_a):\n",
    "    return p_v + p_a - p_v * p_a\n",
    "\n",
    "# Function to calculate integration coefficient\n",
    "def calc_integration(p_observed, p_expected):\n",
    "    return (p_observed - p_expected) / (p_observed + p_expected)\n",
    "\n",
    "# Initialize an empty DataFrame for storing integration coefficients\n",
    "integration_df = pd.DataFrame(columns=['Visual', 'Auditory', 'Integration'])\n",
    "\n",
    "# Iterate through each multisensory stimulus\n",
    "for _, row in df[(df['Visual'] != 0) & (df['Auditory'] != 0)].iterrows():\n",
    "    visual_intensity = row['Visual']\n",
    "    auditory_intensity = row['Auditory']\n",
    "    p_observed = row['Probability']\n",
    "    \n",
    "    # Get the probabilities of the associated unisensory stimuli\n",
    "    p_v = df[(df['Visual'] == visual_intensity) & (df['Auditory'] == 0)]['Probability'].values[0]\n",
    "    p_a = df[(df['Visual'] == 0) & (df['Auditory'] == auditory_intensity)]['Probability'].values[0]\n",
    "    \n",
    "    # Calculate expected probability\n",
    "    p_expected = calc_expected_p(p_v, p_a)\n",
    "    \n",
    "    # Calculate integration coefficient\n",
    "    integration = calc_integration(p_observed, p_expected)\n",
    "    \n",
    "    # Append to the DataFrame\n",
    "    integration_df = integration_df.append({'Visual': visual_intensity, 'Auditory': auditory_intensity, 'Integration': integration}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Pivot the DataFrame to get a matrix form suitable for heatmap\n",
    "integration_matrix = integration_df.pivot(\"Visual\", \"Auditory\", \"Integration\")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "heatmap = sns.heatmap(integration_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "heatmap.set_title('Integration Coefficients Heatmap')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis\n",
    "plt.show()\n",
    "\n",
    "# Initialize an empty DataFrame for storing integration coefficients\n",
    "integration_df = pd.DataFrame(columns=['Visual', 'Auditory', 'Integration'])\n",
    "\n",
    "# Iterate through each multisensory stimulus\n",
    "for _, row in df[(df['Visual'] != 0) & (df['Auditory'] != 0)].iterrows():\n",
    "    visual_intensity = row['Visual']\n",
    "    auditory_intensity = row['Auditory']\n",
    "    p_observed = row['Probability']\n",
    "    \n",
    "    # Get the probabilities of the associated unisensory stimuli\n",
    "    p_v = df[(df['Visual'] == visual_intensity) & (df['Auditory'] == 0)]['Probability'].values[0]\n",
    "    p_a = df[(df['Visual'] == 0) & (df['Auditory'] == auditory_intensity)]['Probability'].values[0]\n",
    "    \n",
    "    # Calculate expected probability\n",
    "    p_expected = max(p_v, p_a)\n",
    "    \n",
    "    # Calculate integration coefficient\n",
    "    integration = calc_integration(p_observed, p_expected)\n",
    "    \n",
    "    # Append to the DataFrame\n",
    "    integration_df = integration_df.append({'Visual': visual_intensity, 'Auditory': auditory_intensity, 'Integration': integration}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Pivot the DataFrame to get a matrix form suitable for heatmap\n",
    "integration_matrix = integration_df.pivot(\"Visual\", \"Auditory\", \"Integration\")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "heatmap = sns.heatmap(integration_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "heatmap.set_title('Integration Coefficients Heatmap')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d644de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the trials to set 'is_estimated_escape'\n",
    "index_start = 0\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1])\n",
    "        #trial_estimated_escapes = trial['estimated_escape'][within_frame_range]\n",
    "        \n",
    "        #trial['is_estimated_escape'] = any(trial_estimated_escapes)\n",
    "        trial['is_estimated_escape'] = within_frame_range.sum() > 0\n",
    "\n",
    "# Initialize a dictionary to hold escape probabilities for each stimulus type\n",
    "escape_probabilities = {}\n",
    "\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    is_estimated_escape_values = []\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        is_estimated_escape_values.append(trial['is_estimated_escape'])\n",
    "    escape_probabilities[stimulus_type] = np.mean(is_estimated_escape_values)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "visual_intensities = []\n",
    "auditory_intensities = []\n",
    "probabilities = []\n",
    "\n",
    "for stimulus_type, probability in escape_probabilities.items():\n",
    "    visual_intensity, auditory_intensity = map(float, stimulus_type.split('_'))\n",
    "    visual_intensities.append(visual_intensity)\n",
    "    auditory_intensities.append(auditory_intensity)\n",
    "    probabilities.append(probability)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Visual': visual_intensities,\n",
    "    'Auditory': auditory_intensities,\n",
    "    'Probability': probabilities\n",
    "})\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.pivot(\"Visual\", \"Auditory\", \"Probability\"), cmap=\"Oranges\", annot=True, cbar_kws={'label': 'Escape Probability'}, vmin=0, vmax=1)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Escape Probability Heatmap\")\n",
    "#plt.savefig(f\"04 - Observed probability matrix.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Function to calculate expected probability\n",
    "def calc_expected_p(p_v, p_a):\n",
    "    return p_v + p_a - p_v * p_a\n",
    "\n",
    "# Function to calculate integration coefficient\n",
    "def calc_integration(p_observed, p_expected):\n",
    "    return (p_observed - p_expected) / (p_observed + p_expected)\n",
    "\n",
    "# Initialize an empty DataFrame for storing integration coefficients\n",
    "integration_df = pd.DataFrame(columns=['Visual', 'Auditory', 'Integration'])\n",
    "\n",
    "# Iterate through each multisensory stimulus\n",
    "for _, row in df[(df['Visual'] != 0) & (df['Auditory'] != 0)].iterrows():\n",
    "    visual_intensity = row['Visual']\n",
    "    auditory_intensity = row['Auditory']\n",
    "    p_observed = row['Probability']\n",
    "    \n",
    "    # Get the probabilities of the associated unisensory stimuli\n",
    "    p_v = df[(df['Visual'] == visual_intensity) & (df['Auditory'] == 0)]['Probability'].values[0]\n",
    "    p_a = df[(df['Visual'] == 0) & (df['Auditory'] == auditory_intensity)]['Probability'].values[0]\n",
    "    \n",
    "    # Calculate expected probability\n",
    "    p_expected = calc_expected_p(p_v, p_a)\n",
    "    \n",
    "    # Calculate integration coefficient\n",
    "    integration = calc_integration(p_observed, p_expected)\n",
    "    \n",
    "    # Append to the DataFrame\n",
    "    integration_df = integration_df.append({'Visual': visual_intensity, 'Auditory': auditory_intensity, 'Integration': integration}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Pivot the DataFrame to get a matrix form suitable for heatmap\n",
    "integration_matrix = integration_df.pivot(\"Visual\", \"Auditory\", \"Integration\")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "heatmap = sns.heatmap(integration_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "heatmap.set_title('Integration Coefficients Heatmap')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis\n",
    "plt.show()\n",
    "\n",
    "# Initialize an empty DataFrame for storing integration coefficients\n",
    "integration_df = pd.DataFrame(columns=['Visual', 'Auditory', 'Integration'])\n",
    "\n",
    "# Iterate through each multisensory stimulus\n",
    "for _, row in df[(df['Visual'] != 0) & (df['Auditory'] != 0)].iterrows():\n",
    "    visual_intensity = row['Visual']\n",
    "    auditory_intensity = row['Auditory']\n",
    "    p_observed = row['Probability']\n",
    "    \n",
    "    # Get the probabilities of the associated unisensory stimuli\n",
    "    p_v = df[(df['Visual'] == visual_intensity) & (df['Auditory'] == 0)]['Probability'].values[0]\n",
    "    p_a = df[(df['Visual'] == 0) & (df['Auditory'] == auditory_intensity)]['Probability'].values[0]\n",
    "    \n",
    "    # Calculate expected probability\n",
    "    p_expected = max(p_v, p_a)\n",
    "    \n",
    "    # Calculate integration coefficient\n",
    "    integration = calc_integration(p_observed, p_expected)\n",
    "    \n",
    "    # Append to the DataFrame\n",
    "    integration_df = integration_df.append({'Visual': visual_intensity, 'Auditory': auditory_intensity, 'Integration': integration}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Pivot the DataFrame to get a matrix form suitable for heatmap\n",
    "integration_matrix = integration_df.pivot(\"Visual\", \"Auditory\", \"Integration\")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "heatmap = sns.heatmap(integration_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "heatmap.set_title('Integration Coefficients Heatmap')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c97798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors based on visual intensity\n",
    "custom_colors = {\n",
    "    \"0.0_0.0\": (0.3, 0.3, 0.3),\n",
    "    \"0.0_1.0\": (0., 0.2        , 1.        ),\n",
    "    \"0.3_0.0\": (1.        , 0.72156863, 0.16862745),\n",
    "    \"0.0_0.3\": (0.        , 0.83529412, 1.        ),\n",
    "    \"1.0_0.0\": (1., 0., 0.),\n",
    "    \n",
    "    \"0.3_0.3\": (0.83529412, 0.56862745, 1),\n",
    "    \"0.3_1.0\": (0.57254902, 0.01176471, 1),\n",
    "    \"1.0_0.3\": (1.        , 0.56862745, 0.75686275),\n",
    "    \"1.0_1.0\": (1.        , 0.        , 0.81568627)\n",
    "}\n",
    "\n",
    "custom_labels = {\n",
    "    \"0.0_0.0\": \"Control\",\n",
    "    \"0.0_1.0\": \"Aud Max\",\n",
    "    \"0.3_0.0\": \"Vis Min\",\n",
    "    \"0.0_0.3\": \"Aud Min\",\n",
    "    \"1.0_0.0\": \"Vis Max\",\n",
    "    \n",
    "    \"0.3_0.3\": \"Vis Min + Aud Min\",\n",
    "    \"0.3_1.0\": \"Vis Min + Aud Max\",\n",
    "    \"1.0_0.3\": \"Vis Max + Aud Min\",\n",
    "    \"1.0_1.0\": \"Vis Max + Aud Max\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the trials to set 'is_estimated_escape'\n",
    "index_start = 0\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1])\n",
    "        #trial_estimated_escapes = trial['estimated_escape'][within_frame_range]\n",
    "        \n",
    "        #trial['is_estimated_escape'] = any(trial_estimated_escapes)\n",
    "        trial['is_estimated_escape'] = within_frame_range.sum() > 0\n",
    "\n",
    "# Initialize a dictionary to hold escape probabilities for each stimulus type\n",
    "escape_probabilities = {}\n",
    "\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    is_estimated_escape_values = []\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        is_estimated_escape_values.append(trial['is_estimated_escape'])\n",
    "    escape_probabilities[stimulus_type] = np.mean(is_estimated_escape_values)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "visual_intensities = []\n",
    "auditory_intensities = []\n",
    "probabilities = []\n",
    "\n",
    "for stimulus_type, probability in escape_probabilities.items():\n",
    "    visual_intensity, auditory_intensity = map(float, stimulus_type.split('_'))\n",
    "    visual_intensities.append(visual_intensity)\n",
    "    auditory_intensities.append(auditory_intensity)\n",
    "    probabilities.append(probability)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Visual': visual_intensities,\n",
    "    'Auditory': auditory_intensities,\n",
    "    'Probability': probabilities\n",
    "})\n",
    "\n",
    "df['CustomColor'] = df.apply(lambda row: custom_colors[f\"{row['Visual']}_{row['Auditory']}\"], axis=1)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(8, 8))\n",
    "heatmap = sns.heatmap(df.pivot(\"Visual\", \"Auditory\", \"Probability\"), annot_kws={\"size\": 24}, cmap=\"Oranges\", cbar=False,annot=True, cbar_kws={'label': 'Escape Probability'}, vmin=0, vmax=1)\n",
    "\n",
    "# Sort unique intensities\n",
    "sorted_visual = sorted(df['Visual'].unique())\n",
    "sorted_auditory = sorted(df['Auditory'].unique())\n",
    "\n",
    "ax = plt.gca()  # Get the current Axes instance\n",
    "\n",
    "for i, vis in enumerate(sorted_visual):  # Reverse to match the inverted y-axis\n",
    "    for j, aud in enumerate(sorted_auditory):\n",
    "        color = custom_colors.get(f\"{vis}_{aud}\", \"white\")\n",
    "        ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=True, color=color))\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Set the fontsize for axis ticks\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), fontsize=14)\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), fontsize=14)\n",
    "\n",
    "# Set the fontsize for axis labels and title\n",
    "heatmap.set_xlabel(\"Auditory\", fontsize=16)\n",
    "heatmap.set_ylabel(\"Visual\", fontsize=16)\n",
    "#heatmap.set_title(\"Escape Probability Heatmap\", fontsize=16)\n",
    "\n",
    "plt.savefig(f\"04 - Observed probability matrix.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to hold escape probabilities and standard errors for each stimulus type\n",
    "escape_probabilities = {}\n",
    "escape_standard_errors = {}\n",
    "\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    is_estimated_escape_values = []\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        is_estimated_escape_values.append(trial['is_estimated_escape'])\n",
    "    \n",
    "    # Compute mean escape probability\n",
    "    escape_probabilities[stimulus_type] = np.mean(is_estimated_escape_values)\n",
    "    \n",
    "    # Compute standard error\n",
    "    std_dev = np.std(is_estimated_escape_values)\n",
    "    sample_size = len(is_estimated_escape_values)\n",
    "    escape_standard_errors[stimulus_type] = std_dev / np.sqrt(sample_size)\n",
    "\n",
    "# Prepare data for plotting\n",
    "stimuli_types = list(escape_probabilities.keys())\n",
    "prob_values = list(escape_probabilities.values())\n",
    "error_values = list(escape_standard_errors.values())\n",
    "\n",
    "# Assuming you have a dictionary called \"custom_colors\" for coloring bars\n",
    "colors = [custom_colors[stim] for stim in stimuli_types]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(stimuli_types, prob_values, color=colors, yerr=error_values, capsize=5)\n",
    "\n",
    "# Setting labels and title\n",
    "plt.xlabel('Stimulus Type', fontsize=16)\n",
    "plt.ylabel('Escape Probability', fontsize=16)\n",
    "plt.title('Escape Probability by Stimulus Type', fontsize=18)\n",
    "\n",
    "# Optionally, rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"Escape_Probability_Bar_Chart_with_SE.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the trials to set 'is_estimated_escape'\n",
    "index_start = 0\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        within_frame_range = (trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1])\n",
    "        #trial_estimated_escapes = trial['estimated_escape'][within_frame_range]\n",
    "        \n",
    "        #trial['is_estimated_escape'] = any(trial_estimated_escapes)\n",
    "        trial['is_estimated_escape'] = within_frame_range.sum() > 0\n",
    "        \n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to collect 'is_estimated_escape' values\n",
    "sat_escape = []\n",
    "no_sat_escape = []\n",
    "\n",
    "# Loop through all trials\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        if 'is_estimated_escape' in trial:\n",
    "            video_name = trial['metadata']['Video']\n",
    "            is_estimated_escape = trial['is_estimated_escape']\n",
    "\n",
    "            # Categorize the trial based on 'sat' in the video name\n",
    "            if \"sat\" in video_name:\n",
    "                sat_escape.append(is_estimated_escape)\n",
    "            else:\n",
    "                no_sat_escape.append(is_estimated_escape)\n",
    "\n",
    "# Convert lists to numpy arrays for easier manipulation\n",
    "sat_escape = np.array(sat_escape)\n",
    "no_sat_escape = np.array(no_sat_escape)\n",
    "\n",
    "# Calculate the number of escapes and non-escapes for each group\n",
    "sat_escape_count = np.sum(sat_escape)\n",
    "sat_no_escape_count = len(sat_escape) - sat_escape_count\n",
    "\n",
    "no_sat_escape_count = np.sum(no_sat_escape)\n",
    "no_sat_no_escape_count = len(no_sat_escape) - no_sat_escape_count\n",
    "\n",
    "# Prepare contingency table\n",
    "contingency_table = [[sat_escape_count, sat_no_escape_count],\n",
    "                     [no_sat_escape_count, no_sat_no_escape_count]]\n",
    "\n",
    "# Perform chi-squared test\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the p-value\n",
    "print(f\"Chi-squared test p-value: {p_value}\")\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Combine the 'is_estimated_escape' data and create group labels\n",
    "all_escape_data = np.concatenate([sat_escape, no_sat_escape])\n",
    "group_labels = np.array([\"sat\"] * len(sat_escape) + [\"no_sat\"] * len(no_sat_escape))\n",
    "\n",
    "# Convert group labels to numerical values: 1 for 'sat' and 0 for 'no_sat'\n",
    "group_numeric = np.where(group_labels == \"sat\", 1, 0)\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X = sm.add_constant(group_numeric)\n",
    "\n",
    "# Fit the binomial GLM\n",
    "glm_binom = sm.GLM(all_escape_data, X, family=sm.families.Binomial())\n",
    "res = glm_binom.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(res.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold escape probabilities for each stimulus type\n",
    "escape_probabilities = {}\n",
    "\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    is_estimated_escape_values = []\n",
    "    for trial in trial_list:\n",
    "        if len(trial['event_features']) == 0 or np.isnan(trial['event_features'].mean()):\n",
    "            continue\n",
    "        \n",
    "        event_clusters = trial['event_clusters']\n",
    "        \n",
    "        if len(event_clusters) == 0 or np.isnan(event_clusters.mean()):\n",
    "            continue\n",
    "        \n",
    "        within_frame_range = (trial['valid_event_times'][event_clusters == 0] >= stim_window[0]) & (trial['valid_event_times'][event_clusters == 0] <= stim_window[1])\n",
    "        \n",
    "        trial['is_estimated_escape'] = within_frame_range.sum() > 0\n",
    "        is_estimated_escape_values.append(trial['is_estimated_escape'])\n",
    "        \n",
    "    escape_probabilities[stimulus_type] = np.mean(is_estimated_escape_values)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "visual_intensities = []\n",
    "auditory_intensities = []\n",
    "probabilities = []\n",
    "\n",
    "for stimulus_type, probability in escape_probabilities.items():\n",
    "    visual_intensity, auditory_intensity = map(float, stimulus_type.split('_'))\n",
    "    visual_intensities.append(visual_intensity)\n",
    "    auditory_intensities.append(auditory_intensity)\n",
    "    probabilities.append(probability)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Visual': visual_intensities,\n",
    "    'Auditory': auditory_intensities,\n",
    "    'Probability': probabilities\n",
    "})\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.pivot(\"Visual\", \"Auditory\", \"Probability\"), cmap=\"Oranges\", annot=True, cbar_kws={'label': 'Escape Probability'}, vmin=0, vmax=1)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Escape Probability Heatmap\")\n",
    "plt.savefig(f\"04b - Observed probability matrix only for C-start cluster events.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize a dictionary to store x, y coordinates grouped by category\n",
    "# coords_by_category = {\n",
    "#     'Control': ([], []),\n",
    "#     'Visual': ([], []),\n",
    "#     'Auditory': ([], []),\n",
    "#     'Multisensory': ([], [])\n",
    "# }\n",
    "\n",
    "# colors = {'Control': \"gray\", 'Visual': \"red\", 'Auditory': \"blue\", 'Multisensory': \"magenta\"}\n",
    "\n",
    "# # Loop through each stimulus type in trials\n",
    "# for stim_type, trial_list in trials.items():\n",
    "#     # Determine the category for this stimulus type\n",
    "#     visual, auditory = map(float, stim_type.split('_'))\n",
    "    \n",
    "#     if visual == 0.0 and auditory == 0.0:\n",
    "#         category = 'Control'\n",
    "#     elif visual != 0.0 and auditory == 0.0:\n",
    "#         category = 'Visual'\n",
    "#     elif visual == 0.0 and auditory != 0.0:\n",
    "#         category = 'Auditory'\n",
    "#     else:\n",
    "#         category = 'Multisensory'\n",
    "\n",
    "#     # Initialize lists to store x, y coordinates for this category\n",
    "#     x_coords, y_coords = coords_by_category[category]\n",
    "\n",
    "#     # Loop through each trial in the list of trials for this stimulus type\n",
    "#     for trial in trial_list:\n",
    "# #         # Check if this trial has the 'estimated_escape' and 'is_estimated_escape' keys\n",
    "# #         if 'is_estimated_escape' in trial:\n",
    "# #             # If this trial is labeled as positive for estimated escapes\n",
    "# #             if trial['is_estimated_escape']:\n",
    "#                 # Get the estimated escapes\n",
    "#                 #estimated_escapes = trial['estimated_escape']\n",
    "#                 # Get the valid event times\n",
    "#         if \"low_D_representation\" not in trial:\n",
    "#             continue\n",
    "        \n",
    "#         valid_event_times = trial['valid_event_times']\n",
    "#         # Get the low_D_representation\n",
    "#         low_D_rep = trial['low_D_representation']\n",
    "\n",
    "#         # Loop through each valid event to check the conditions\n",
    "#         for i, (event_time, low_D) in enumerate(zip(valid_event_times, low_D_rep)):\n",
    "#             # Check if the event is an estimated escape and falls within the relevant time window\n",
    "#             #if is_escape and (stim_window[0] <= event_time <= stim_window[1]):\n",
    "#             if stim_window[0] <= event_time <= stim_window[1]:\n",
    "#                 # Append the x and y coordinates to the lists\n",
    "#                 x_coords.append(low_D[0])\n",
    "#                 y_coords.append(low_D[1])\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Loop through each category to plot its events\n",
    "# for category, (x, y) in coords_by_category.items():\n",
    "#     if category == \"Multisensory\" or category == \"Control\":\n",
    "#         continue\n",
    "    \n",
    "#     jitter_amount = 0.02  # You can adjust this value as needed\n",
    "#     x_jittered = np.array(x) + np.random.uniform(-jitter_amount, jitter_amount, len(x))\n",
    "#     y_jittered = np.array(y) + np.random.uniform(-jitter_amount, jitter_amount, len(y))\n",
    "#     plt.scatter(x_jittered, y_jittered, label=category, color=colors[category], s=100, alpha=0.6)\n",
    "\n",
    "# plt.xlabel('UMAP Dimension 1')\n",
    "# plt.ylabel('UMAP Dimension 2')\n",
    "# plt.title('Scatter plot of estimated escapes in relevant time window')\n",
    "# plt.legend(title='Category')\n",
    "# plt.savefig(f\"05 - 2D representation of escape events by stimulus type.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors based on visual intensity\n",
    "custom_colors = {\n",
    "    \"0.0_0.0\": (0.3, 0.3, 0.3),\n",
    "    \"0.0_1.0\": (0., 0.2        , 1.        ),\n",
    "    \"0.3_0.0\": (1.        , 0.72156863, 0.16862745),\n",
    "    \"0.0_0.3\": (0.        , 0.83529412, 1.        ),\n",
    "    \"1.0_0.0\": (1., 0., 0.),\n",
    "    \n",
    "    \"0.3_0.3\": (0.83529412, 0.56862745, 1),\n",
    "    \"0.3_1.0\": (0.57254902, 0.01176471, 1),\n",
    "    \"1.0_0.3\": (1.        , 0.56862745, 0.75686275),\n",
    "    \"1.0_1.0\": (1.        , 0.        , 0.81568627)\n",
    "}\n",
    "\n",
    "custom_labels = {\n",
    "    \"0.0_0.0\": \"Control\",\n",
    "    \"0.0_1.0\": \"Aud Max\",\n",
    "    \"0.3_0.0\": \"Vis Min\",\n",
    "    \"0.0_0.3\": \"Aud Min\",\n",
    "    \"1.0_0.0\": \"Vis Max\",\n",
    "    \n",
    "    \"0.3_0.3\": \"Vis Min + Aud Min\",\n",
    "    \"0.3_1.0\": \"Vis Min + Aud Max\",\n",
    "    \"1.0_0.3\": \"Vis Max + Aud Min\",\n",
    "    \"1.0_1.0\": \"Vis Max + Aud Max\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(r\"C:\\Users\\PC\\Desktop\\Resultados\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patheffects import withStroke\n",
    "\n",
    "# Initialize a dictionary to store x, y coordinates grouped by category\n",
    "coords_by_category = {}\n",
    "colors = {}\n",
    "\n",
    "# Loop through each stimulus type in trials\n",
    "for stim_type, trial_list in trials.items():\n",
    "    # Determine the category for this stimulus type\n",
    "    visual, auditory = map(float, stim_type.split('_'))\n",
    "    \n",
    "    if visual == 0.0 and auditory == 0.0:\n",
    "        category = stim_type\n",
    "        if category not in coords_by_category:\n",
    "            coords_by_category[category] = ([], [])\n",
    "            colors[category] = custom_colors[stim_type]\n",
    "    elif visual != 0.0 and auditory == 0.0:\n",
    "        category = stim_type\n",
    "        if category not in coords_by_category:\n",
    "            coords_by_category[category] = ([], [])\n",
    "            colors[category] = custom_colors[stim_type]\n",
    "    elif visual == 0.0 and auditory != 0.0:\n",
    "        category = stim_type\n",
    "        if category not in coords_by_category:\n",
    "            coords_by_category[category] = ([], [])\n",
    "            colors[category] = custom_colors[stim_type]\n",
    "\n",
    "    # Skip multisensory\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Initialize lists to store x, y coordinates for this category\n",
    "    x_coords, y_coords = coords_by_category[category]\n",
    "\n",
    "    for trial in trial_list:\n",
    "        if \"low_D_representation\" not in trial:\n",
    "            continue\n",
    "        \n",
    "        valid_event_times = trial['valid_event_times']\n",
    "        low_D_rep = trial['low_D_representation']\n",
    "\n",
    "        for i, (event_time, low_D) in enumerate(zip(valid_event_times, low_D_rep)):\n",
    "            if stim_window[0] <= event_time <= stim_window[1]:\n",
    "                x_coords.append(low_D[0])\n",
    "                y_coords.append(low_D[1])\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "unisensory_centroids = {}\n",
    "\n",
    "# Plot centroids and events\n",
    "for category, (x, y) in coords_by_category.items():\n",
    "    jitter_amount = 0.0\n",
    "    x_jittered = np.array(x) + np.random.uniform(-jitter_amount, jitter_amount, len(x))\n",
    "    y_jittered = np.array(y) + np.random.uniform(-jitter_amount, jitter_amount, len(y))\n",
    "    plt.scatter(x_jittered, y_jittered, label=custom_labels[category], color=colors[category], s=130, alpha=0.8,edgecolor='black')\n",
    "    \n",
    "    if category != \"0.0_0.0\":\n",
    "        centroid_x = np.median(x)\n",
    "        centroid_y = np.median(y)\n",
    "        unisensory_centroids[category] = (centroid_x, centroid_y)\n",
    "        \n",
    "        plt.text(centroid_x, centroid_y, 'x', color=colors[category], fontsize=30, ha='center', va='center',path_effects=[withStroke(linewidth=5, foreground='black')])\n",
    "\n",
    "plt.xlabel('Dimension 1', fontsize=16)\n",
    "plt.ylabel('Dimension 2', fontsize=16)\n",
    "plt.title('Scatter plot of estimated escapes in relevant time window', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlim(-0.55,0.8)\n",
    "plt.ylim(-0.5,0.6)\n",
    "plt.savefig(f\"05 - 2D representation of escape events by stimulus type.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250410b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop over each category to plot its density\n",
    "for category, (x, y) in coords_by_category.items():\n",
    "    if custom_labels[category] == \"Control\":\n",
    "        continue\n",
    "    sns.kdeplot(x, bw_adjust=1.0, label=custom_labels[category], color=custom_colors[category], fill=True, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Dimension 1', fontsize=16)\n",
    "plt.title('Density plot along Dimension 1', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlim(-0.55, 0.8)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"06 - Density plot along Dimension 1.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop over each category to plot its density\n",
    "for category, (x, y) in coords_by_category.items():\n",
    "    if custom_labels[category] == \"Control\":\n",
    "        continue\n",
    "    sns.kdeplot(y, bw_adjust=1.0, label=custom_labels[category], color=custom_colors[category], fill=True, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Dimension 2', fontsize=16)\n",
    "plt.title('Density plot along Dimension 2', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlim(-0.5, 0.6)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"06 - Density plot along Dimension 2.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ced8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a dictionary to store x, y coordinates grouped by category\n",
    "coords_by_category = {}\n",
    "\n",
    "# Loop through each stimulus type in trials\n",
    "for stim_type, trial_list in trials.items():\n",
    "    # Determine the category for this stimulus type\n",
    "    visual, auditory = map(float, stim_type.split('_'))\n",
    "\n",
    "    # Focus only on multisensory stimuli\n",
    "    if visual != 0.0 and auditory != 0.0:\n",
    "        category = stim_type\n",
    "        coords_by_category[category] = ([], [])\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Initialize lists to store x, y coordinates for this category\n",
    "    x_coords, y_coords = coords_by_category[category]\n",
    "\n",
    "    for trial in trial_list:\n",
    "        if \"low_D_representation\" not in trial:\n",
    "            continue\n",
    "\n",
    "        valid_event_times = trial['valid_event_times']\n",
    "        low_D_rep = trial['low_D_representation']\n",
    "\n",
    "        for i, (event_time, low_D) in enumerate(zip(valid_event_times, low_D_rep)):\n",
    "            if stim_window[0] <= event_time <= stim_window[1]:\n",
    "                x_coords.append(low_D[0])\n",
    "                y_coords.append(low_D[1])\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot centroids and events\n",
    "for category, (x, y) in coords_by_category.items():\n",
    "    jitter_amount = 0.02\n",
    "    x_jittered = np.array(x) + np.random.uniform(-jitter_amount, jitter_amount, len(x))\n",
    "    y_jittered = np.array(y) + np.random.uniform(-jitter_amount, jitter_amount, len(y))\n",
    "\n",
    "    plt.scatter(x_jittered, y_jittered, label=custom_labels[category], color=custom_colors[category], s=130, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "for category, (x, y) in unisensory_centroids.items():\n",
    "    plt.text(x, y, 'x', color=colors[category], fontsize=30, ha='center', va='center',path_effects=[withStroke(linewidth=5, foreground='black')])\n",
    "    \n",
    "\n",
    "plt.xlabel('Dimension 1', fontsize=16)\n",
    "plt.ylabel('Dimension 2', fontsize=16)\n",
    "plt.title('Scatter plot of multisensory events by specific stimulus', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlim(-0.55,0.8)\n",
    "plt.ylim(-0.5,0.6)\n",
    "plt.savefig(f\"07 - Multisensory 2D representation by specific stimulus.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75629f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop over each category to plot its density\n",
    "for category, (x, y) in coords_by_category.items():\n",
    "    if custom_labels[category] == \"Control\":\n",
    "        continue\n",
    "    sns.kdeplot(x, bw_adjust=1.0, label=custom_labels[category], color=custom_colors[category], fill=True, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Dimension 1', fontsize=16)\n",
    "plt.title('Density plot along Dimension 1', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlim(-0.55, 0.8)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"07 - Density plot along Dimension 1.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop over each category to plot its density\n",
    "for category, (x, y) in coords_by_category.items():\n",
    "    if custom_labels[category] == \"Control\":\n",
    "        continue\n",
    "    sns.kdeplot(y, bw_adjust=1.0, label=custom_labels[category], color=custom_colors[category], fill=True, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Dimension 2', fontsize=16)\n",
    "plt.title('Density plot along Dimension 2', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlim(-0.5, 0.6)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"07 - Density plot along Dimension 2.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([255, 0, 208])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabbfc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize a dictionary to store x, y coordinates specifically for multisensory stimuli\n",
    "# coords_by_multisensory_stim = {}\n",
    "\n",
    "# # # Define custom colors based on visual intensity\n",
    "# # custom_colors = {\n",
    "# #     \"0.3_0.3\": (0.83529412, 0.56862745, 1),\n",
    "# #     \"0.3_1.0\": (0.57254902, 0.01176471, 1),\n",
    "# #     \"1.0_0.3\": (1.        , 0.56862745, 0.75686275),\n",
    "# #     \"1.0_1.0\": (0.98039216, 0.00784314, 0.38431373)\n",
    "# # }\n",
    "\n",
    "# # Loop through each stimulus type in trials\n",
    "# for stim_type, trial_list in trials.items():\n",
    "#     visual, auditory = map(float, stim_type.split('_'))\n",
    "\n",
    "#     # Only populate for multisensory stimuli\n",
    "#     if visual != 0.0 and auditory != 0.0:\n",
    "#         if stim_type not in coords_by_multisensory_stim:\n",
    "#             coords_by_multisensory_stim[stim_type] = ([], [])\n",
    "#         x_coords, y_coords = coords_by_multisensory_stim[stim_type]\n",
    "\n",
    "#         # Loop through each trial in the list of trials for this stimulus type\n",
    "#         for trial in trial_list:\n",
    "#             if 'low_D_representation' in trial:\n",
    "#                 #estimated_escapes = trial['estimated_escape']\n",
    "#                 valid_event_times = trial['valid_event_times']\n",
    "#                 low_D_rep = trial['low_D_representation']\n",
    "\n",
    "#                 for i, (event_time, low_D) in enumerate(zip(valid_event_times, low_D_rep)):\n",
    "#                     if stim_window[0] <= event_time <= stim_window[1]:\n",
    "#                         x_coords.append(low_D[0])\n",
    "#                         y_coords.append(low_D[1])\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Loop through each multisensory type to plot its events\n",
    "# for stim_type, (x, y) in coords_by_multisensory_stim.items():\n",
    "#     plt.scatter(x, y, label=stim_type, color=custom_colors[stim_type], s=100, alpha=0.8)\n",
    "\n",
    "# plt.xlabel('UMAP Dimension 1')\n",
    "# plt.ylabel('UMAP Dimension 2')\n",
    "# plt.title('Scatter plot of estimated escapes in relevant time window for multisensory stimuli')\n",
    "# plt.legend(title='Category')\n",
    "# plt.savefig(f\"06 - 2D representation of multisensory escape events by stimulus type.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store counts\n",
    "cluster_counts = {}\n",
    "\n",
    "# Define custom colors based on visual intensity and add colors for unisensory and control\n",
    "custom_colors = {\n",
    "    \"0.3_0.3\": (0.83529412, 0.56862745, 1),\n",
    "    \"0.3_1.0\": (0.57254902, 0.01176471, 1),\n",
    "    \"1.0_0.3\": (1.        , 0.56862745, 0.75686275),\n",
    "    \"1.0_1.0\": (0.98039216, 0.00784314, 0.38431373),\n",
    "    \"0.0_0.0\": \"gray\",\n",
    "    \"0.3_0.0\": (1.        , 0.61568627, 0.45098039),\n",
    "    \"1.0_0.0\": (1.        , 0.30196078, 0.        ),\n",
    "    \"0.0_0.3\": (0.49019608, 0.70980392, 1.        ),\n",
    "    \"0.0_1.0\": (0.        , 0.43137255, 1.        )\n",
    "}\n",
    "\n",
    "# Loop through each stimulus type in trials\n",
    "for stim_type, trial_list in trials.items():\n",
    "    total_events = 0\n",
    "    cluster_0_events = 0\n",
    "    \n",
    "    for trial in trial_list:\n",
    "        if 'is_estimated_escape' in trial and trial['is_estimated_escape']:\n",
    "            valid_event_times = trial['valid_event_times']\n",
    "            event_clusters = trial['event_clusters']\n",
    "            \n",
    "            for event_time, cluster in zip(valid_event_times, event_clusters):\n",
    "                if stim_window[0] <= event_time <= stim_window[1]:\n",
    "                    total_events += 1\n",
    "                    if cluster == 0:\n",
    "                        cluster_0_events += 1\n",
    "    \n",
    "    # Store the counts for this stimulus type\n",
    "    cluster_counts[stim_type] = (cluster_0_events, total_events)\n",
    "\n",
    "# Calculate proportions\n",
    "proportions = {}\n",
    "for stim_type, (cluster_0, total) in cluster_counts.items():\n",
    "    if total > 0:\n",
    "        proportions[stim_type] = cluster_0 / total\n",
    "    else:\n",
    "        proportions[stim_type] = 0\n",
    "\n",
    "# Define a custom order for the stimulus types\n",
    "custom_order = [\"0.0_0.0\", \"0.3_0.0\", \"1.0_0.0\", \"0.0_0.3\", \"0.0_1.0\", \"0.3_0.3\", \"0.3_1.0\", \"1.0_0.3\", \"1.0_1.0\"]\n",
    "\n",
    "# Filter out any stim types not in our dataset (this is optional but good for robustness)\n",
    "custom_order = [stim for stim in custom_order if stim in proportions]\n",
    "\n",
    "# Get the proportion values and colors in the custom order\n",
    "proportion_values_ordered = [proportions[stim] for stim in custom_order]\n",
    "colors_ordered = [custom_colors[stim] for stim in custom_order]\n",
    "\n",
    "# Plot the ordered proportions\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.bar(custom_order, proportion_values_ordered, color=colors_ordered)\n",
    "ax.set_xlabel('Stimulus Type')\n",
    "ax.set_ylabel('Proportion in C-Start Cluster')\n",
    "ax.set_title('Proportion of Events in C-Start Cluster by Stimulus Type')\n",
    "plt.savefig(f\"07 - Proportion of events in C-start cluster by stimulus type.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a dictionary to store centroids for each stimulus type\n",
    "centroids = {}\n",
    "\n",
    "for stim_type, trials_i in trials.items():\n",
    "    # Initialize a list to store low_D_representation for this stimulus type\n",
    "    low_D_points = []\n",
    "    \n",
    "    for trial in trials_i:\n",
    "        if 'is_estimated_escape' in trial and trial['is_estimated_escape']:\n",
    "            valid_indices = np.where((trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1]))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                low_D_points.extend(trial['low_D_representation'][valid_indices])\n",
    "    \n",
    "    # Convert to numpy array for easier calculations\n",
    "    low_D_points = np.array(low_D_points)\n",
    "    \n",
    "    # Calculate centroid for this stimulus type\n",
    "    centroid = np.median(low_D_points, axis=0)\n",
    "    centroids[stim_type] = centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89259899",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_dict = {}\n",
    "\n",
    "for stim_type, trials_i in trials.items():\n",
    "    visual_intensity, auditory_intensity = map(float, stim_type.split('_'))\n",
    "    \n",
    "    # Skip unisensory and null stimuli\n",
    "    if visual_intensity == 0.0 or auditory_intensity == 0.0:\n",
    "        continue\n",
    "    \n",
    "    # Initialize a list to store low_D_representation for this stimulus type\n",
    "    low_D_points = []\n",
    "    \n",
    "    for trial in trials_i:\n",
    "        if 'is_estimated_escape' in trial and trial['is_estimated_escape']:\n",
    "            valid_indices = np.where((trial['valid_event_times'] >= stim_window[0]) & (trial['valid_event_times'] <= stim_window[1]))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                low_D_points.extend(trial['low_D_representation'][valid_indices])\n",
    "    \n",
    "    # Convert to numpy array for easier calculations\n",
    "    low_D_points = np.array(low_D_points)\n",
    "    \n",
    "    visual_centroid = centroids[f\"{visual_intensity}_0.0\"]\n",
    "    auditory_centroid = centroids[f\"0.0_{auditory_intensity}\"]\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    for low_D_rep in low_D_points:\n",
    "        # Calculate distances to the visual and auditory centroids\n",
    "        distance_to_visual = np.linalg.norm(low_D_rep - visual_centroid)\n",
    "        distance_to_auditory = np.linalg.norm(low_D_rep - auditory_centroid)\n",
    "\n",
    "        distances.append([distance_to_visual, distance_to_auditory])\n",
    "    \n",
    "    # Convert distances to a numpy array for easier manipulation\n",
    "    distances = np.array(distances)\n",
    "\n",
    "    # Calculate the standard deviations for visual and auditory distances\n",
    "#     std_visual = np.std(distances[:, 0])\n",
    "#     std_auditory = np.std(distances[:, 1])\n",
    "\n",
    "#     # Normalize the distances\n",
    "#     distances[:, 0] /= std_visual\n",
    "#     distances[:, 1] /= std_auditory\n",
    "\n",
    "    # Add the normalized distances to your distances_dict\n",
    "    distances_dict[stim_type] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scatterplot\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Loop through each multisensory stimulus type and plot\n",
    "for stim_type, distances in distances_dict.items():\n",
    "    ax.scatter(distances[:, 0], distances[:, 1], label=custom_labels[stim_type], color=custom_colors[stim_type], s=130, alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Distance to Visual Centroid',fontsize=14)\n",
    "ax.set_ylabel('Distance to Auditory Centroid',fontsize=14)\n",
    "#ax.set_title('Distances to Visual and Auditory Centroids for Multisensory Stimuli')\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "# Get the limits for both axes\n",
    "x_lim = ax.get_xlim()\n",
    "y_lim = ax.get_ylim()\n",
    "\n",
    "# Calculate the common limits for the identity line\n",
    "common_lim = [min(x_lim[0], y_lim[0]), max(x_lim[1], y_lim[1])]\n",
    "\n",
    "# Plot the identity line\n",
    "ax.plot(common_lim, common_lim, '--', linewidth=2, color='gray', label='Identity Line')\n",
    "\n",
    "# Optionally, set the axes limits back to the common limits\n",
    "ax.set_xlim(common_lim)\n",
    "ax.set_ylim(common_lim)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(f\"08 - Distance from multisensory events to unisensory centroids.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop through each multisensory stimulus type and plot its density\n",
    "for stim_type, distances in distances_dict.items():\n",
    "    sns.kdeplot(distances[:, 0], bw_adjust=1.0, label=custom_labels[stim_type], color=custom_colors[stim_type], fill=True, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Distance to Visual Centroid', fontsize=16)\n",
    "plt.title('Density plot for Distance to Visual Centroid', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlim(common_lim)  # Assuming common_lim is defined in your workspace\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"08 - Density plot for Distance to Visual Centroid.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop through each multisensory stimulus type and plot its density\n",
    "for stim_type, distances in distances_dict.items():\n",
    "    sns.kdeplot(distances[:, 1], bw_adjust=1.0, label=custom_labels[stim_type], color=custom_colors[stim_type], fill=True, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Distance to Auditory Centroid', fontsize=16)\n",
    "plt.title('Density plot for Distance to Auditory Centroid', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlim(common_lim)  # Assuming common_lim is defined in your workspace\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"08 - Density plot for Distance to Auditory Centroid.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead30aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Stimulus_Type', 'Component', 'Distance'])\n",
    "\n",
    "# Loop through the distances_dict to populate the DataFrame\n",
    "for stim_type, distances in distances_dict.items():\n",
    "    for distance in distances:\n",
    "        df = df.append({'Stimulus_Type': stim_type, 'Component': 'Visual', 'Distance': distance[0]}, ignore_index=True)\n",
    "        df = df.append({'Stimulus_Type': stim_type, 'Component': 'Auditory', 'Distance': distance[1]}, ignore_index=True)\n",
    "\n",
    "df['Stimulus_Type'] = pd.Categorical(df['Stimulus_Type'], categories=sorted(distances_dict.keys()), ordered=True)\n",
    "df.sort_values('Stimulus_Type', inplace=True)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Create boxplot + swarmplot for Visual Component\n",
    "sns.boxplot(x='Stimulus_Type', y='Distance', data=df[df['Component'] == 'Visual'], ax=axs[0], palette=custom_colors, hue_order=sorted(distances_dict.keys()))\n",
    "sns.swarmplot(x='Stimulus_Type', y='Distance', data=df[df['Component'] == 'Visual'], ax=axs[0], color='black')\n",
    "axs[0].set_title('Visual Component', fontsize=14)\n",
    "axs[0].set_ylabel('Distance to Visual Centroid', fontsize=14)\n",
    "axs[0].set_xlabel('')\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "axs[0].set_xticklabels([custom_labels[stim_type] for stim_type in sorted(distances_dict.keys())], rotation=10)\n",
    "\n",
    "# Create boxplot + swarmplot for Auditory Component\n",
    "sns.boxplot(x='Stimulus_Type', y='Distance', data=df[df['Component'] == 'Auditory'], ax=axs[1], palette=custom_colors, hue_order=sorted(distances_dict.keys()))\n",
    "sns.swarmplot(x='Stimulus_Type', y='Distance', data=df[df['Component'] == 'Auditory'], ax=axs[1], color='black')\n",
    "axs[1].set_title('Auditory Component', fontsize=14)\n",
    "axs[1].set_ylabel('Distance to Auditory Centroid', fontsize=14)\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "axs[1].set_xlabel('')\n",
    "axs[1].set_xticklabels([custom_labels[stim_type] for stim_type in sorted(distances_dict.keys())], rotation=10)\n",
    "# Show the plot\n",
    "plt.savefig(f\"09 - Distance from multisensory events to unisensory centroids as difference in distributions.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe23d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Stimulus_Type', 'Component', 'Distance'])\n",
    "\n",
    "used_keys = []\n",
    "# Loop through the distances_dict to populate the DataFrame\n",
    "for stim_type, distances in distances_dict.items():\n",
    "    for distance in distances:\n",
    "        if \"Aud Min\" not in custom_labels[stim_type]:\n",
    "            used_keys.append(stim_type)\n",
    "            df = df.append({'Stimulus_Type': stim_type, 'Component': 'Visual', 'Distance': distance[0]}, ignore_index=True)\n",
    "            df = df.append({'Stimulus_Type': stim_type, 'Component': 'Auditory', 'Distance': distance[1]}, ignore_index=True)\n",
    "\n",
    "df['Stimulus_Type'] = pd.Categorical(df['Stimulus_Type'], categories=sorted(distances_dict.keys()), ordered=True)\n",
    "df['Stimulus_Type'].cat.remove_unused_categories(inplace=True)\n",
    "df.sort_values('Stimulus_Type', inplace=True)\n",
    "\n",
    "used_keys = np.unique(used_keys)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Create boxplot + swarmplot for Visual Component\n",
    "sns.boxplot(x='Stimulus_Type', y='Distance', data=df[df['Component'] == 'Visual'], ax=axs[0], palette=custom_colors, hue_order=sorted(used_keys))\n",
    "sns.swarmplot(x='Stimulus_Type', y='Distance', data=df[df['Component'] == 'Visual'], ax=axs[0], color='black')\n",
    "axs[0].set_title('Visual Component', fontsize=16)\n",
    "axs[0].set_ylabel('Distance to Visual Centroid', fontsize=16)\n",
    "axs[0].set_xlabel('')\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "#axs[0].set_xticks(range(len(sorted(distances_dict.keys()))))\n",
    "#axs[0].set_xticklabels([custom_labels[stim_type] for stim_type in sorted(distances_dict.keys())], rotation=10)\n",
    "\n",
    "\n",
    "# Create boxplot + swarmplot for Auditory Component\n",
    "sns.boxplot(x='Stimulus_Type', y='Distance', data=df[df['Component'] == 'Auditory'], ax=axs[1], palette=custom_colors, hue_order=sorted(used_keys))\n",
    "sns.swarmplot(x='Stimulus_Type', y='Distance', data=df[df['Component'] == 'Auditory'], ax=axs[1], color='black')\n",
    "axs[1].set_title('Auditory Component', fontsize=16)\n",
    "axs[1].set_ylabel('Distance to Auditory Centroid', fontsize=16)\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=16)\n",
    "axs[1].set_xlabel('')\n",
    "#axs[1].set_xticklabels([custom_labels[stim_type] for stim_type in sorted(distances_dict.keys())], rotation=10)\n",
    "\n",
    "\n",
    "axs[0].set_xticks(range(len(sorted(used_keys))))\n",
    "axs[0].set_xticklabels([custom_labels[stim_type] for stim_type in sorted(used_keys)], rotation=0)\n",
    "\n",
    "axs[1].set_xticks(range(len(sorted(used_keys))))\n",
    "axs[1].set_xticklabels([custom_labels[stim_type] for stim_type in sorted(used_keys)], rotation=0)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(f\"09b - Distance from multisensory events to unisensory centroids as difference in distributions.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Initialize a dictionary to store the p-values for each component\n",
    "p_values = {'Visual': {}, 'Auditory': {}}\n",
    "\n",
    "components = ['Visual', 'Auditory']\n",
    "\n",
    "for component in components:\n",
    "    stimulus_types = df[df['Component'] == component]['Stimulus_Type'].unique()\n",
    "    \n",
    "    for i, stim_type1 in enumerate(stimulus_types):\n",
    "        for j, stim_type2 in enumerate(stimulus_types):\n",
    "            if i < j:  # To avoid duplicate and self-comparisons\n",
    "                sample1 = df[(df['Component'] == component) & (df['Stimulus_Type'] == stim_type1)]['Distance']\n",
    "                sample2 = df[(df['Component'] == component) & (df['Stimulus_Type'] == stim_type2)]['Distance']\n",
    "                \n",
    "                # Mann-Whitney U test\n",
    "                _, p_value = mannwhitneyu(sample1, sample2)\n",
    "                \n",
    "                # Store the p-value\n",
    "                comparison = f\"{stim_type1} vs {stim_type2}\"\n",
    "                p_values[component][comparison] = p_value\n",
    "\n",
    "# Number of comparisons\n",
    "num_comparisons = len(p_values['Visual'])  # This will be the same for 'Visual' and 'Auditory'\n",
    "\n",
    "# Bonferroni correction\n",
    "corrected_p_values = {component: {comparison: p_value * num_comparisons for comparison, p_value in comparisons.items()} \n",
    "                      for component, comparisons in p_values.items()}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the corrected p-values to DataFrames for easy viewing\n",
    "df_visual = pd.DataFrame(list(corrected_p_values['Visual'].items()), columns=['Comparison', 'Corrected_P_Value'])\n",
    "df_auditory = pd.DataFrame(list(corrected_p_values['Auditory'].items()), columns=['Comparison', 'Corrected_P_Value'])\n",
    "\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "# Add 'Significance' column to the Visual DataFrame\n",
    "df_visual['Significance'] = df_visual['Corrected_P_Value'].apply(lambda x: '*' if x <= alpha else '')\n",
    "\n",
    "# Add 'Significance' column to the Auditory DataFrame\n",
    "df_auditory['Significance'] = df_auditory['Corrected_P_Value'].apply(lambda x: '*' if x <= alpha else '')\n",
    "\n",
    "\n",
    "# Display the tables\n",
    "print(\"Visual Component Comparisons:\")\n",
    "print(df_visual)\n",
    "print(\"\\nAuditory Component Comparisons:\")\n",
    "print(df_auditory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import euclidean\n",
    "\n",
    "# # Initialize a 2D dictionary to store distances\n",
    "# distances_matrix = {}\n",
    "\n",
    "# for row_stim, row_trials in trials.items():\n",
    "#     distances_matrix[row_stim] = {}\n",
    "#     for col_stim, col_centroid in centroids.items():\n",
    "#         distances_matrix[row_stim][col_stim] = []\n",
    "        \n",
    "#         for trial in row_trials:\n",
    "#             if 'estimated_escape' in trial and 'is_estimated_escape' in trial and trial['is_estimated_escape']:\n",
    "#                 valid_indices = np.where((trial['valid_event_times'] >= 10000) & (trial['valid_event_times'] <= 11300))[0]\n",
    "#                 if len(valid_indices) > 0:\n",
    "#                     for point in trial['low_D_representation'][valid_indices]:\n",
    "#                         distance = euclidean(point, col_centroid)\n",
    "#                         distances_matrix[row_stim][col_stim].append(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Order rows and columns\n",
    "# ordered_stim_types = ['0.0_0.0', '0.3_0.0', '1.0_0.0', '0.0_0.3', '0.0_1.0', '0.3_0.3', '1.0_0.3', '0.3_1.0', '1.0_1.0']\n",
    "\n",
    "# # Create an empty numpy array to store mean distances\n",
    "# num_stim_types = len(ordered_stim_types)\n",
    "# mean_distances_array = np.zeros((num_stim_types, num_stim_types))\n",
    "\n",
    "# # Fill in the numpy array with mean distances\n",
    "# for i, row_stim in enumerate(ordered_stim_types):\n",
    "#     for j, col_stim in enumerate(ordered_stim_types):\n",
    "#         distance_list = distances_matrix[row_stim][col_stim]\n",
    "#         if len(distance_list) > 0:\n",
    "#             mean_distance = np.mean(distance_list)\n",
    "#         else:\n",
    "#             mean_distance = np.nan  # Or any value to represent empty cells\n",
    "#         mean_distances_array[i, j] = mean_distance\n",
    "\n",
    "# # Plot heatmap\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(mean_distances_array, annot=True, fmt=\".2f\", cmap='coolwarm', \n",
    "#             xticklabels=ordered_stim_types, yticklabels=ordered_stim_types)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14679abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty numpy array to store the counts\n",
    "# count_distances_array = np.zeros((num_stim_types, num_stim_types), dtype=int)\n",
    "\n",
    "# # Fill in the numpy array with the number of distances\n",
    "# for i, row_stim in enumerate(ordered_stim_types):\n",
    "#     for j, col_stim in enumerate(ordered_stim_types):\n",
    "#         distance_list = distances_matrix[row_stim][col_stim]\n",
    "#         count_distances_array[i, j] = len(distance_list)\n",
    "\n",
    "# # Plot heatmap for the counts\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(count_distances_array, annot=True, fmt=\"d\", cmap='coolwarm', \n",
    "#             xticklabels=ordered_stim_types, yticklabels=ordered_stim_types)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d534f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize a dictionary to store the centroids for each stimulus type\n",
    "# centroids = {}\n",
    "\n",
    "# # Loop through each stimulus type\n",
    "# for stim_type, trials in trials.items():\n",
    "#     low_D_reps = []\n",
    "    \n",
    "#     # Loop through each trial\n",
    "#     for trial in trials:\n",
    "#         # Check if the trial is estimated as an escape\n",
    "#         if 'estimated_escape' in trial and trial['is_estimated_escape']:\n",
    "#             # Loop through each valid event time\n",
    "#             for i, event_time in enumerate(trial['valid_event_times']):\n",
    "#                 # Check if the event time falls within the stimulation window\n",
    "#                 if 10000 <= event_time <= 11300:\n",
    "#                     # Check if this event is estimated to be an escape\n",
    "#                     if trial['estimated_escape'][i]:\n",
    "#                         low_D_reps.append(trial['low_D_representation'][i])\n",
    "                        \n",
    "#     # Calculate the centroid for this stimulus type by taking the median\n",
    "#     centroid = np.median(low_D_reps, axis=0)\n",
    "#     centroids[stim_type] = centroid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Initialize a dictionary to collect low-D representations for each stimulus type\n",
    "# lowD_per_stimulus = defaultdict(list)\n",
    "\n",
    "# # Iterate through all the stimulus types and their corresponding trials\n",
    "# for stimulus_type, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "#         # Check if the trial is labeled as an estimated escape\n",
    "#         if 'estimated_escape' in trial and trial['is_estimated_escape']:\n",
    "#             # Get valid event times\n",
    "#             valid_event_times = trial['valid_event_times']\n",
    "#             # Filter events that fall within the time window\n",
    "#             valid_indices = np.where((valid_event_times >= 10000) & (valid_event_times <= 11300))[0]\n",
    "#             # Get low_D_representation for those events\n",
    "#             relevant_lowD = trial['low_D_representation'][valid_indices]\n",
    "#             # Append these to the list corresponding to this stimulus type\n",
    "#             lowD_per_stimulus[stimulus_type].extend(relevant_lowD)\n",
    "\n",
    "# # Calculate the centroid for each stimulus type\n",
    "# centroids = {}\n",
    "# for stimulus_type, lowD_list in lowD_per_stimulus.items():\n",
    "#     if len(lowD_list) > 0:\n",
    "#         centroids[stimulus_type] = np.median(lowD_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf534fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a dictionary to collect distance arrays for each multisensory stimulus\n",
    "# distances_per_multisensory = {}\n",
    "\n",
    "# # Iterate through all the stimulus types and their corresponding centroids\n",
    "# for stimulus_type, centroid in centroids.items():\n",
    "#     visual_intensity, auditory_intensity = map(float, stimulus_type.split('_'))\n",
    "#     # Check if it is a multisensory stimulus\n",
    "#     if visual_intensity > 0.0 and auditory_intensity > 0.0:\n",
    "#         # Initialize a list to collect distances\n",
    "#         distances_list = []\n",
    "#         # Iterate through the trials for this stimulus type\n",
    "#         for trial in trials[stimulus_type]:\n",
    "#             if 'low_D_representation' in trial and 'estimated_escape' in trial:\n",
    "#                 # Filter for events that are estimated escapes\n",
    "#                 estimated_escapes = trial['estimated_escape']\n",
    "#                 # Get valid event times and filter for relevant window\n",
    "#                 valid_event_times = trial['valid_event_times']\n",
    "                \n",
    "#                 print(estimated_escapes.shape, valid_event_times.shape)\n",
    "                \n",
    "#                 valid_indices = np.where((valid_event_times >= 10000) & (valid_event_times <= 11300) & estimated_escapes)[0]\n",
    "#                 # Continue only if valid_indices is not empty\n",
    "#                 if len(valid_indices) > 0:\n",
    "#                     # Get the low_D_representation for these filtered events\n",
    "#                     lowD_filtered_events = trial['low_D_representation'][valid_indices]\n",
    "#                     # Calculate the distance to the visual and auditory centroids\n",
    "#                     visual_centroid = centroids[f\"{visual_intensity}_0.0\"]\n",
    "#                     auditory_centroid = centroids[f\"0.0_{auditory_intensity}\"]\n",
    "#                     distances_to_visual = np.linalg.norm(lowD_filtered_events - visual_centroid, axis=1)\n",
    "#                     distances_to_auditory = np.linalg.norm(lowD_filtered_events - auditory_centroid, axis=1)\n",
    "#                     # Combine these into a 2D array for this trial\n",
    "#                     distances_trial = np.column_stack((distances_to_visual, distances_to_auditory))\n",
    "#                     distances_list.append(distances_trial)\n",
    "#         # Combine these into a 2D array for this multisensory stimulus\n",
    "#         if distances_list:\n",
    "#             distances_per_multisensory[stimulus_type] = np.vstack(distances_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the base colors\n",
    "# gray = (0.5, 0.5, 0.5)\n",
    "# light_red = (1.0, 0.7, 0.7)\n",
    "# dark_red = (1.0, 0.0, 0.0)\n",
    "# light_blue = (0.7, 0.7, 1.0)\n",
    "# dark_blue = (0.0, 0.0, 1.0)\n",
    "\n",
    "# # Initialize the stim_colors dictionary\n",
    "# stim_colors = {}\n",
    "\n",
    "# # Populate the dictionary with colors\n",
    "# stim_colors['0.0_0.0'] = gray\n",
    "# stim_colors['0.3_0.0'] = light_red\n",
    "# stim_colors['1.0_0.0'] = dark_red\n",
    "# stim_colors['0.0_0.3'] = light_blue\n",
    "# stim_colors['0.0_1.0'] = dark_blue\n",
    "\n",
    "# # Multisensory stimuli combinations\n",
    "# stim_colors['0.3_0.3'] = (0.85, 0.7, 0.85)  # Light mix of red and blue\n",
    "# stim_colors['1.0_0.3'] = (1.0, 0.0, 0.3)  # More red, less blue\n",
    "# stim_colors['0.3_1.0'] = (0.3, 0.0, 1.0)  # More blue, less red\n",
    "# stim_colors['1.0_1.0'] = (1.0, 0.0, 1.0)  # Full red and blue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eb945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize the scatterplot\n",
    "# plt.figure(figsize=(12,12))\n",
    "\n",
    "# # Add points for each multisensory stimulus\n",
    "# for stimulus_type, distances in distances_per_multisensory.items():\n",
    "#     plt.scatter(distances[:, 0], distances[:, 1], label=stimulus_type, color=stim_colors[stimulus_type])\n",
    "\n",
    "# # Add axis labels and legend\n",
    "# plt.xlabel('Distance to Visual Centroid')\n",
    "# plt.ylabel('Distance to Auditory Centroid')\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12,12))\n",
    "\n",
    "# for stimulus_type, lowD_list in lowD_per_stimulus.items():\n",
    "#     lowD_array = np.array(lowD_list)\n",
    "#     plt.scatter(lowD_array[:, 0], lowD_array[:, 1], label=stimulus_type, color=stim_colors[stimulus_type])\n",
    "    \n",
    "# for stimulus_type, centroid in centroids.items():\n",
    "#     plt.scatter(centroid[0], centroid[1], marker='x', s=250, color=stim_colors[stimulus_type])\n",
    "\n",
    "# plt.xlabel('Low-D Dimension 1')\n",
    "# plt.ylabel('Low-D Dimension 2')\n",
    "# plt.title('Centroids in Low-D Space')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439dcb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "\n",
    "# for stimulus_type, lowD_list in lowD_per_stimulus.items():\n",
    "#     lowD_array = np.array(lowD_list)\n",
    "#     plt.scatter(lowD_array[:, 0], lowD_array[:, 1], label=stimulus_type)\n",
    "\n",
    "# plt.xlabel('Low-D Dimension 1')\n",
    "# plt.ylabel('Low-D Dimension 2')\n",
    "# plt.title('Low-D Representations by Stimulus Type')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effcbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# all_event_features_escapes = []\n",
    "# all_is_escape_escapes = []\n",
    "\n",
    "# # Assume `trials` is your dictionary containing all the data\n",
    "# for stimulus_type, trial_list in trials.items():\n",
    "#     for trial in trial_list:\n",
    "        \n",
    "#         if len(trial['estimated_escape']) == 0 or trial['estimated_escape'].sum() == 0:\n",
    "#             continue\n",
    "        \n",
    "#         all_event_features_escapes.append(trial['event_features'][trial['estimated_escape']])\n",
    "#         all_is_escape_escapes.append(trial['is_escape'][trial['estimated_escape']])\n",
    "\n",
    "# all_event_features_escapes = np.vstack(all_event_features_escapes)\n",
    "# all_is_escape_escapes = np.concatenate(all_is_escape_escapes)\n",
    "\n",
    "# print(all_event_features_escapes.shape, all_is_escape_escapes.shape, all_is_escape_escapes.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming trials is your dictionary containing all the trial information.\n",
    "# Loop through each stimulus type in the trials dictionary.\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    # Loop through each trial in the list of trials for this stimulus type.\n",
    "    for trial in trial_list:\n",
    "        # Initialize the 'fish_position' dictionary within each trial.\n",
    "        trial['fish_position'] = {\n",
    "            'mean_eye_position': [],\n",
    "            'distance_to_bottom': [],\n",
    "            'distance_to_left': [],\n",
    "            'vector_to_head': [],\n",
    "            'angle_to_bottom': [],\n",
    "            'angle_to_left': []\n",
    "        }\n",
    "        \n",
    "        # If there are no events, the lists will remain empty.\n",
    "        if len(trial['valid_event_times']) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Loop through each valid event time in the trial.\n",
    "        for event_time in trial['valid_event_times']:\n",
    "            # Extract the tracking data for this event time.\n",
    "            tracking_data = trial['tracking_data'].loc[event_time]\n",
    "            \n",
    "            # Calculate the mean position of the eyes.\n",
    "            mean_eye_x = np.mean([tracking_data['eyeL_x'], tracking_data['eyeR_x']])\n",
    "            mean_eye_y = np.mean([tracking_data['eyeL_y'], tracking_data['eyeR_y']])\n",
    "            mean_eye_position = (mean_eye_x, mean_eye_y)\n",
    "            \n",
    "            # Calculate the distance from the mean eye position to the bottom and left of the frame.\n",
    "            distance_to_bottom = 720 - mean_eye_y\n",
    "            distance_to_left = mean_eye_x\n",
    "            \n",
    "            # Calculate the vector pointing from the vejigaP point to the fish head (mean position of the eyes).\n",
    "            vector_to_head = (mean_eye_x - tracking_data['vejigaP_x'], mean_eye_y - tracking_data['vejigaP_y'])\n",
    "            \n",
    "            # Calculate the angles.\n",
    "            angle_to_bottom = np.arctan2(vector_to_head[1], vector_to_head[0]) * 180 / np.pi\n",
    "            angle_to_left = np.arctan2(vector_to_head[0], vector_to_head[1]) * 180 / np.pi\n",
    "            \n",
    "            # Save these calculated values into the 'fish_position' dictionary.\n",
    "            trial['fish_position']['mean_eye_position'].append(mean_eye_position)\n",
    "            trial['fish_position']['distance_to_bottom'].append(distance_to_bottom)\n",
    "            trial['fish_position']['distance_to_left'].append(distance_to_left)\n",
    "            trial['fish_position']['vector_to_head'].append(vector_to_head)\n",
    "            trial['fish_position']['angle_to_bottom'].append(angle_to_bottom)\n",
    "            trial['fish_position']['angle_to_left'].append(angle_to_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f764f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize empty lists to hold data for scatter plots and histograms\n",
    "mean_eye_positions = []\n",
    "stimulus_types_for_scatter = []\n",
    "distances_to_bottom = []\n",
    "distances_to_left = []\n",
    "angles_to_bottom = []\n",
    "angles_to_left = []\n",
    "stimulus_types_for_angles = []\n",
    "\n",
    "# Loop through each stimulus type in the trials dictionary to populate the above lists\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    for trial in trial_list:\n",
    "        fish_position = trial.get('fish_position', {})\n",
    "        \n",
    "        mean_eye_positions.extend(fish_position.get('mean_eye_position', []))\n",
    "        stimulus_types_for_scatter.extend([stimulus_type] * len(fish_position.get('mean_eye_position', [])))\n",
    "        \n",
    "        distances_to_bottom.extend(fish_position.get('distance_to_bottom', []))\n",
    "        distances_to_left.extend(fish_position.get('distance_to_left', []))\n",
    "        \n",
    "        angles_to_bottom.extend(fish_position.get('angle_to_bottom', []))\n",
    "        angles_to_left.extend(fish_position.get('angle_to_left', []))\n",
    "        stimulus_types_for_angles.extend([stimulus_type] * len(fish_position.get('angle_to_bottom', [])))\n",
    "\n",
    "# Convert mean_eye_positions to x and y coordinates for easier plotting\n",
    "mean_eye_positions_x = [pos[0] for pos in mean_eye_positions]\n",
    "mean_eye_positions_y = [pos[1] for pos in mean_eye_positions]\n",
    "\n",
    "# Scatter plot for mean eye positions, colored by stimulus type\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=mean_eye_positions_x, y=mean_eye_positions_y, hue=stimulus_types_for_scatter, palette=custom_colors)\n",
    "plt.title(\"Scatter Plot of Mean Eye Positions by Stimulus Type\")\n",
    "plt.xlabel(\"Mean Eye X Position\")\n",
    "plt.ylabel(\"Mean Eye Y Position\")\n",
    "plt.legend(title='Stimulus Type', labels=[custom_labels.get(stim, stim) for stim in set(stimulus_types_for_scatter)])\n",
    "plt.show()\n",
    "\n",
    "# Histograms for distances\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(distances_to_bottom, bins=30)\n",
    "plt.title(\"Histogram of Distances to Bottom\")\n",
    "plt.xlabel(\"Distance to Bottom\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(distances_to_left, bins=30)\n",
    "plt.title(\"Histogram of Distances to Left\")\n",
    "plt.xlabel(\"Distance to Left\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Histograms for angles\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(angles_to_bottom, bins=30)\n",
    "plt.title(\"Histogram of Angles to Bottom\")\n",
    "plt.xlabel(\"Angle to Bottom\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(angles_to_left, bins=30)\n",
    "plt.title(\"Histogram of Angles to Left\")\n",
    "plt.xlabel(\"Angle to Left\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for angles, colored by stimulus type\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=angles_to_bottom, y=angles_to_left, hue=stimulus_types_for_angles, palette=custom_colors)\n",
    "plt.title(\"Scatter Plot of Angles by Stimulus Type\")\n",
    "plt.xlabel(\"Angle to Bottom\")\n",
    "plt.ylabel(\"Angle to Left\")\n",
    "plt.legend(title='Stimulus Type', labels=[custom_labels.get(stim, stim) for stim in set(stimulus_types_for_angles)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bed432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Initialize empty lists to collect data\n",
    "low_D_dimension1 = []\n",
    "distances_to_bottom = []\n",
    "distances_to_left = []\n",
    "angles_to_bottom = []\n",
    "angles_to_left = []\n",
    "visual_intensities = []\n",
    "auditory_intensities = []\n",
    "\n",
    "# Populate the lists with data from all trials\n",
    "for stimulus_type, trial_list in trials.items():\n",
    "    visual_intensity, auditory_intensity = map(float, stimulus_type.split('_'))\n",
    "    \n",
    "    for trial in trial_list:\n",
    "        fish_position = trial.get('fish_position', {})\n",
    "        low_D_rep = trial.get('low_D_representation', [])\n",
    "        \n",
    "        num_events = len(fish_position.get('distance_to_bottom', []))\n",
    "        if num_events > 0:\n",
    "            low_D_dimension1.extend(low_D_rep[:, 0])  # Assuming low_D_rep is a numpy array\n",
    "            \n",
    "            distances_to_bottom.extend(fish_position['distance_to_bottom'])\n",
    "            distances_to_left.extend(fish_position['distance_to_left'])\n",
    "            angles_to_bottom.extend(fish_position['angle_to_bottom'])\n",
    "            angles_to_left.extend(fish_position['angle_to_left'])\n",
    "            \n",
    "            visual_intensities.extend([visual_intensity] * num_events)\n",
    "            auditory_intensities.extend([auditory_intensity] * num_events)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'low_D_dimension1': low_D_dimension1,\n",
    "    'distance_to_bottom': distances_to_bottom,\n",
    "    'distance_to_left': distances_to_left,\n",
    "    'angle_to_bottom': angles_to_bottom,\n",
    "    'angle_to_left': angles_to_left,\n",
    "    'visual_intensity': visual_intensities,\n",
    "    'auditory_intensity': auditory_intensities\n",
    "})\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "data = sm.add_constant(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6416fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GLM model\n",
    "model = sm.GLM(data['low_D_dimension1'], data[['const', 'distance_to_bottom', 'distance_to_left', 'angle_to_bottom', 'angle_to_left']])\n",
    "result = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(\"GLM Model Summary:\")\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a61e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GLM model\n",
    "model = sm.GLM(data['low_D_dimension1'], data[['const', 'distance_to_bottom']])\n",
    "result = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(\"GLM Model Summary:\")\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for multisensory stimuli where the visual component is 1.0\n",
    "filtered_data = data[(data['visual_intensity'] > 0.0) & (data['auditory_intensity'] > 0.0)]\n",
    "\n",
    "# Create the filtered GLM model\n",
    "filtered_model = sm.GLM(filtered_data['low_D_dimension1'], filtered_data[['const', 'distance_to_bottom', 'distance_to_left', 'angle_to_bottom', 'angle_to_left']])\n",
    "filtered_result = filtered_model.fit()\n",
    "\n",
    "# Print the summary of the filtered model\n",
    "print(\"Filtered GLM Model Summary:\")\n",
    "print(filtered_result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for multisensory stimuli where the visual component is 1.0\n",
    "filtered_data = data[(data['visual_intensity'] > 0.0) & (data['auditory_intensity'] > 0.0)]\n",
    "\n",
    "# Create the filtered GLM model\n",
    "filtered_model = sm.GLM(filtered_data['low_D_dimension1'], filtered_data[['const', 'distance_to_bottom']])\n",
    "filtered_result = filtered_model.fit()\n",
    "\n",
    "# Print the summary of the filtered model\n",
    "print(\"Filtered GLM Model Summary:\")\n",
    "print(filtered_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Distance to bottom\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(filtered_data['distance_to_bottom'], filtered_data['low_D_dimension1'], alpha=0.5)\n",
    "plt.title('Distance to Bottom vs. Low-D Dimension 1')\n",
    "plt.xlabel('Distance to Bottom')\n",
    "plt.ylabel('Low-D Dimension 1')\n",
    "\n",
    "# Distance to left\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(filtered_data['distance_to_left'], filtered_data['low_D_dimension1'], alpha=0.5)\n",
    "plt.title('Distance to Left vs. Low-D Dimension 1')\n",
    "plt.xlabel('Distance to Left')\n",
    "plt.ylabel('Low-D Dimension 1')\n",
    "\n",
    "# Angle to bottom\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(filtered_data['angle_to_bottom'], filtered_data['low_D_dimension1'], alpha=0.5)\n",
    "plt.title('Angle to Bottom vs. Low-D Dimension 1')\n",
    "plt.xlabel('Angle to Bottom')\n",
    "plt.ylabel('Low-D Dimension 1')\n",
    "\n",
    "# Angle to left\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(filtered_data['angle_to_left'], filtered_data['low_D_dimension1'], alpha=0.5)\n",
    "plt.title('Angle to Left vs. Low-D Dimension 1')\n",
    "plt.xlabel('Angle to Left')\n",
    "plt.ylabel('Low-D Dimension 1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53462e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for multisensory stimuli where the visual component is 1.0\n",
    "filtered_data = data[(data['visual_intensity'] > 0.0)]\n",
    "\n",
    "# Create the filtered GLM model\n",
    "filtered_model = sm.GLM(filtered_data['low_D_dimension1'], filtered_data[['const', 'distance_to_bottom']])\n",
    "filtered_result = filtered_model.fit()\n",
    "\n",
    "# Print the summary of the filtered model\n",
    "print(\"Filtered GLM Model Summary:\")\n",
    "print(filtered_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54056727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for multisensory stimuli where the visual component is 1.0\n",
    "filtered_data = data[(data['visual_intensity'] == 0.0)]\n",
    "\n",
    "# Create the filtered GLM model\n",
    "filtered_model = sm.GLM(filtered_data['low_D_dimension1'], filtered_data[['const', 'distance_to_bottom']])\n",
    "filtered_result = filtered_model.fit()\n",
    "\n",
    "# Print the summary of the filtered model\n",
    "print(\"Filtered GLM Model Summary:\")\n",
    "print(filtered_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_heading_and_C1(trials):\n",
    "    for stimulus_type, stimulus_trials in trials.items():\n",
    "        heading_angles_list = []\n",
    "        C1_durations = []\n",
    "        C1_angles = []\n",
    "        max_angular_velocities = []\n",
    "        \n",
    "        for trial in stimulus_trials:\n",
    "            tracking_data = trial['tracking_data']\n",
    "            valid_event_times = trial['valid_event_times']\n",
    "            \n",
    "            heading_angles_per_trial = []\n",
    "            C1_durations_per_trial = []\n",
    "            C1_angles_per_trial = []\n",
    "            max_angular_velocities_per_trial = []\n",
    "            \n",
    "            for event_time in valid_event_times:\n",
    "                # Calculate the initial vector at the event time\n",
    "                initial_vector = np.array([\n",
    "                    tracking_data.loc[event_time, 'vejigaP_x'] - tracking_data.loc[event_time, 'eye_mean_x'],\n",
    "                    tracking_data.loc[event_time, 'vejigaP_y'] - tracking_data.loc[event_time, 'eye_mean_y']\n",
    "                ])\n",
    "                \n",
    "                # Initialize list to store heading angles for this event\n",
    "                heading_angles = []\n",
    "                \n",
    "                for i in range(30):  # For each of the first 30 frames after the event\n",
    "                    current_frame = event_time + i\n",
    "                    \n",
    "                    # Calculate the current vector\n",
    "                    current_vector = np.array([\n",
    "                        tracking_data.loc[current_frame, 'vejigaP_x'] - tracking_data.loc[current_frame, 'eye_mean_x'],\n",
    "                        tracking_data.loc[current_frame, 'vejigaP_y'] - tracking_data.loc[current_frame, 'eye_mean_y']\n",
    "                    ])\n",
    "                    \n",
    "                    # Calculate the angle between the initial and current vector\n",
    "                    angle = np.arccos(np.dot(initial_vector, current_vector) / \n",
    "                                      (np.linalg.norm(initial_vector) * np.linalg.norm(current_vector)))\n",
    "                    \n",
    "                    heading_angles.append(angle)\n",
    "                \n",
    "                # Save the heading angles for this event\n",
    "                heading_angles_per_trial.append(np.array(heading_angles))\n",
    "                \n",
    "                # Calculate C1 duration and angle\n",
    "                derivative = np.diff(np.abs(heading_angles))\n",
    "                C1_end_frame = np.where(derivative <= 0)[0][0] - 1\n",
    "                C1_durations_per_trial.append(C1_end_frame)\n",
    "                C1_angles_per_trial.append(np.abs(heading_angles[C1_end_frame]))\n",
    "                \n",
    "                # Calculate angular velocity (change in heading angle between each frame)\n",
    "                angular_velocity = np.diff(heading_angles)\n",
    "\n",
    "                # Store the max angular velocity for this event\n",
    "                max_angular_velocities_per_trial.append(np.max(np.abs(angular_velocity)))\n",
    "                \n",
    "            # Save all event data for this trial\n",
    "            heading_angles_list.append(heading_angles_per_trial)\n",
    "            C1_durations.append(C1_durations_per_trial)\n",
    "            C1_angles.append(C1_angles_per_trial)\n",
    "            max_angular_velocities.append(max_angular_velocities_per_trial)\n",
    "            \n",
    "        for i, trial in enumerate(stimulus_trials):\n",
    "            trial['heading_angles'] = heading_angles_list[i]\n",
    "            trial['C1_durations'] = C1_durations[i]\n",
    "            trial['C1_angles'] = C1_angles[i]\n",
    "            trial['max_angular_velocities'] = max_angular_velocities[i]\n",
    "\n",
    "# Assume `trials` is your dictionary containing all the trial data\n",
    "calculate_heading_and_C1(trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb1351",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Conversion factor from frames to milliseconds\n",
    "frame_to_ms = 1000 / 437\n",
    "\n",
    "def visualize_data(trials, custom_colors):\n",
    "    # Plot 10 random heading angle time series\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i in range(10):\n",
    "        # Ensure we only choose trials that have at least one valid event\n",
    "        while True:\n",
    "            stimulus_type, stimulus_trials = random.choice(list(trials.items()))\n",
    "            trial = random.choice(stimulus_trials)\n",
    "            if len(trial['valid_event_times']) > 0:\n",
    "                break\n",
    "        event_index = random.randint(0, len(trial['valid_event_times']) - 1)\n",
    "        # In the section where you plot the 10 random heading angle time series\n",
    "        plt.xlabel('Time (ms)')\n",
    "\n",
    "        # Convert frame numbers to milliseconds\n",
    "        time_in_ms = np.arange(30) * frame_to_ms\n",
    "        plt.plot(time_in_ms, trial['heading_angles'][event_index], label=f\"Stimulus: {stimulus_type}, Event: {event_index}\")\n",
    "\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Heading Angle (radians)')\n",
    "    plt.title('Randomly Selected Heading Angle Time Series')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare data for histograms\n",
    "    all_C1_durations = []\n",
    "    all_C1_angles = []\n",
    "    cluster_0_durations = []\n",
    "    cluster_0_angles = []\n",
    "    cluster_neg1_durations = []\n",
    "    cluster_neg1_angles = []\n",
    "    \n",
    "    for stimulus_type, stimulus_trials in trials.items():\n",
    "        for trial in stimulus_trials:\n",
    "            all_C1_durations.extend(trial['C1_durations'])\n",
    "            all_C1_angles.extend(trial['C1_angles'])\n",
    "            \n",
    "            event_clusters = trial['event_clusters']\n",
    "            for i, cluster in enumerate(event_clusters):\n",
    "                if cluster == 0:\n",
    "                    cluster_0_durations.append(trial['C1_durations'][i])\n",
    "                    cluster_0_angles.append(trial['C1_angles'][i])\n",
    "                elif cluster == -1:\n",
    "                    cluster_neg1_durations.append(trial['C1_durations'][i])\n",
    "                    cluster_neg1_angles.append(trial['C1_angles'][i])\n",
    "\n",
    "    # Plot histograms for all C1 durations and angles\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Define common bins for C1 durations and angles\n",
    "    bin_edges_durations = np.linspace(min(all_C1_durations), max(all_C1_durations), 16) * frame_to_ms\n",
    "    bin_edges_angles = np.linspace(min(all_C1_angles), max(all_C1_angles), 31)\n",
    "\n",
    "    # Convert C1 durations to milliseconds\n",
    "    all_C1_durations_ms = np.array(all_C1_durations) * frame_to_ms\n",
    "    cluster_0_durations_ms = np.array(cluster_0_durations) * frame_to_ms\n",
    "    cluster_neg1_durations_ms = np.array(cluster_neg1_durations) * frame_to_ms\n",
    "\n",
    "    # Use the common bins in histograms\n",
    "    plt.hist(all_C1_durations_ms, bins=bin_edges_durations, color='b', alpha=0.7, label='C1 Duration')\n",
    "    plt.xlabel('C1 Duration (ms)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of C1 Durations')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(all_C1_angles, bins=30, color='r', alpha=0.7, label='C1 Angle')\n",
    "    plt.xlabel('C1 Angle (radians)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of C1 Angles')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Create mirrored histograms\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    axes[0].hist(cluster_0_durations_ms, bins=bin_edges_durations, color='g', alpha=0.7, label='Cluster 0', density=True)\n",
    "    axes[0].hist(cluster_neg1_durations_ms, bins=bin_edges_durations, color='m', alpha=0.7, label='Cluster -1', density=True)\n",
    "    axes[0].set_xlabel('C1 Duration (ms)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Mirrored Histogram of C1 Durations by Cluster')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].hist(cluster_0_angles, bins=16, color='g', alpha=0.7, label='Cluster 0', density=True)\n",
    "    axes[1].hist(cluster_neg1_angles, bins=16, color='m', alpha=0.7, label='Cluster -1', density=True)\n",
    "    axes[1].set_xlabel('C1 Angle (radians)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Mirrored Histogram of C1 Angles by Cluster')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Assume `trials` is your dictionary containing all the trial data\n",
    "# Assume `custom_colors` is your custom colors dictionary\n",
    "# calculate_heading_and_C1(trials)  # Uncomment if not run previously\n",
    "visualize_data(trials, custom_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion factors\n",
    "frame_to_ms = 437 / 1000\n",
    "radian_to_degree = 180 / np.pi\n",
    "\n",
    "# Initialize lists to store max angular velocities\n",
    "cluster_0_max_angular_velocities = []\n",
    "cluster_neg1_max_angular_velocities = []\n",
    "\n",
    "# Loop over trials to collect data\n",
    "for stimulus_type, stimulus_trials in trials.items():\n",
    "    for trial in stimulus_trials:\n",
    "        max_angular_velocities = np.array(trial['max_angular_velocities'])  # in radians/frame\n",
    "        event_clusters = trial['event_clusters']\n",
    "        \n",
    "        for i, cluster in enumerate(event_clusters):\n",
    "            max_angular_velocity_deg_per_ms = max_angular_velocities[i] * radian_to_degree * frame_to_ms\n",
    "            if cluster == 0:\n",
    "                cluster_0_max_angular_velocities.append(max_angular_velocity_deg_per_ms)\n",
    "            elif cluster == -1:\n",
    "                cluster_neg1_max_angular_velocities.append(max_angular_velocity_deg_per_ms)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(cluster_0_max_angular_velocities, bins=30, alpha=0.7, label='Cluster 0', color='g', density=True)\n",
    "plt.hist(cluster_neg1_max_angular_velocities, bins=30, alpha=0.7, label='Cluster -1', color='m', density=True)\n",
    "plt.xlabel('Max Angular Velocity (degrees/ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Max Angular Velocity by Cluster')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_stats(trials):\n",
    "    # Conversion factors\n",
    "    frame_to_ms = 1000 / 437\n",
    "    radian_to_degree = 180 / np.pi\n",
    "\n",
    "    # Initialize lists to collect data for each cluster\n",
    "    cluster_0_C1_angles = []\n",
    "    cluster_0_C1_durations = []\n",
    "    cluster_0_max_angular_velocities = []\n",
    "\n",
    "    cluster_neg1_C1_angles = []\n",
    "    cluster_neg1_C1_durations = []\n",
    "    cluster_neg1_max_angular_velocities = []\n",
    "\n",
    "    # Loop over trials to collect data\n",
    "    for stimulus_type, stimulus_trials in trials.items():\n",
    "        for trial in stimulus_trials:\n",
    "            C1_angles = np.array(trial['C1_angles'])\n",
    "            C1_durations = np.array(trial['C1_durations']) * frame_to_ms  # convert to ms\n",
    "            max_angular_velocities = np.array(trial['max_angular_velocities']) * radian_to_degree / frame_to_ms  # convert to degrees/ms\n",
    "            event_clusters = trial['event_clusters']\n",
    "            \n",
    "            for i, cluster in enumerate(event_clusters):\n",
    "                if cluster == 0:\n",
    "                    cluster_0_C1_angles.append(C1_angles[i] * radian_to_degree)  # convert to degrees\n",
    "                    cluster_0_C1_durations.append(C1_durations[i])\n",
    "                    cluster_0_max_angular_velocities.append(max_angular_velocities[i])\n",
    "                elif cluster == -1:\n",
    "                    cluster_neg1_C1_angles.append(C1_angles[i] * radian_to_degree)  # convert to degrees\n",
    "                    cluster_neg1_C1_durations.append(C1_durations[i])\n",
    "                    cluster_neg1_max_angular_velocities.append(max_angular_velocities[i])\n",
    "\n",
    "    # Calculate and print statistics for each cluster\n",
    "    for cluster, data in {'Cluster 0': [cluster_0_C1_angles, cluster_0_C1_durations, cluster_0_max_angular_velocities], \n",
    "                          'Cluster -1': [cluster_neg1_C1_angles, cluster_neg1_C1_durations, cluster_neg1_max_angular_velocities]}.items():\n",
    "        print(f\"Statistics for {cluster}:\")\n",
    "        print(f\"  Mean C1 Angle: {np.nanmean(data[0]):.2f} degrees, Std Dev: {np.nanstd(data[0]):.2f} degrees\")\n",
    "        print(f\"  Mean C1 Duration: {np.nanmean(data[1]):.2f} ms, Std Dev: {np.nanstd(data[1]):.2f} ms\")\n",
    "        print(f\"  Mean Max Angular Velocity: {np.nanmean(data[2]):.2f} degrees/ms, Std Dev: {np.nanstd(data[2]):.2f} degrees/ms\")\n",
    "\n",
    "# Run the function\n",
    "# Assume `trials` is your dictionary containing all the trial data\n",
    "calculate_stats(trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738dba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel('G:\\\\My Drive\\\\Proyectos\\\\Zebrafish Multisensory Integration\\\\Cstart Escape Response and Integration - Behavioral Paradigm\\\\Análisis\\\\F_stim_data.xlsx')\n",
    "\n",
    "# Loop through the DataFrame and update the 'trials' dictionary\n",
    "for index, row in df.iterrows():\n",
    "    trial_number = row['Trial number']\n",
    "    F_stim = row['F_stim']\n",
    "    \n",
    "    # Loop through trials dictionary to find the matching trial number and update it\n",
    "    for stimulus_type, stimulus_trials in trials.items():\n",
    "        for trial in stimulus_trials:\n",
    "            if trial['metadata']['Trial'] == trial_number:\n",
    "                trial['metadata']['F_stim'] = F_stim\n",
    "                break  # No need to search further once we've found and updated the trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579fd5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_response_latency(trials):\n",
    "    for stimulus_type, stimulus_trials in trials.items():\n",
    "        for trial in stimulus_trials:\n",
    "            F_stim = trial['metadata'].get('F_stim', None)\n",
    "            valid_event_times = trial.get('valid_event_times', [])\n",
    "            \n",
    "            if F_stim is not None and len(valid_event_times) > 0:\n",
    "                closest_event = min(valid_event_times, key=lambda x: abs(x - F_stim))\n",
    "                \n",
    "                # Check if the closest event is closer than 2000 frames\n",
    "                if abs(closest_event - F_stim) < 2000:\n",
    "                    trial['response_latency'] = closest_event - F_stim\n",
    "                else:\n",
    "                    trial['response_latency'] = np.nan\n",
    "            else:\n",
    "                trial['response_latency'] = np.nan\n",
    "\n",
    "# Run the function to populate the 'response_latency' in each trial\n",
    "calculate_response_latency(trials)\n",
    "\n",
    "global_min = -1500\n",
    "global_max = 1500\n",
    "\n",
    "# Create uniform bins using global min and max\n",
    "bins = np.linspace(global_min, global_max, 50)  # Change 50 to the number of bins you want\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(len(trials), 1, figsize=(10, 40), sharex=True)\n",
    "\n",
    "for i, (stimulus_type, stimulus_trials) in enumerate(trials.items()):\n",
    "    latencies = [trial['response_latency'] for trial in stimulus_trials if not np.isnan(trial['response_latency'])]\n",
    "    axs[i].hist(latencies, bins=bins, color=custom_colors.get(stimulus_type, 'b'))\n",
    "    axs[i].set_title(f\"Response Latency for {custom_labels.get(stimulus_type, stimulus_type)}\")\n",
    "    axs[i].set_xlabel(\"Response Latency (frames)\")\n",
    "    axs[i].set_ylabel(\"Frequency\")\n",
    "    axs[i].set_xlim([global_min, global_max])  # Set the same x-axis limits\n",
    "    axs[i].set_ylim([0, 40])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722395ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global_min = 0\n",
    "global_max = 400\n",
    "\n",
    "# Create uniform bins using global min and max\n",
    "bins = np.linspace(global_min, global_max, 20)  # Change 50 to the number of bins you want\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(len(trials), 1, figsize=(10, 40), sharex=True)\n",
    "\n",
    "for i, (stimulus_type, stimulus_trials) in enumerate(trials.items()):\n",
    "    latencies = [trial['response_latency'] for trial in stimulus_trials if not np.isnan(trial['response_latency'])]\n",
    "    axs[i].hist(latencies, bins=bins, color=custom_colors.get(stimulus_type, 'b'))\n",
    "    axs[i].set_title(f\"Response Latency for {custom_labels.get(stimulus_type, stimulus_type)}\")\n",
    "    axs[i].set_xlabel(\"Response Latency (frames)\")\n",
    "    axs[i].set_ylabel(\"Frequency\")\n",
    "    axs[i].set_xlim([global_min, global_max])  # Set the same x-axis limits\n",
    "    axs[i].set_ylim([0, 40])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to collect latencies for each cluster\n",
    "cluster_0_latencies = []\n",
    "cluster_neg1_latencies = []\n",
    "\n",
    "# Loop through the trials to collect data\n",
    "for stimulus_type, stimulus_trials in trials.items():\n",
    "    for trial in stimulus_trials:\n",
    "        F_stim = trial['metadata'].get('F_stim', None)\n",
    "        valid_event_times = trial.get('valid_event_times', [])\n",
    "        event_clusters = trial.get('event_clusters', [])\n",
    "\n",
    "        if F_stim is not None:\n",
    "            for i, event_time in enumerate(valid_event_times):\n",
    "                latency = event_time - F_stim\n",
    "                \n",
    "                if event_clusters[i] == 0:\n",
    "                    cluster_0_latencies.append(latency)\n",
    "                elif event_clusters[i] == -1:\n",
    "                    cluster_neg1_latencies.append(latency)\n",
    "\n",
    "# Determine global min and max latencies for plotting\n",
    "global_min = -1500\n",
    "global_max = 1500\n",
    "\n",
    "# Create uniform bins using global min and max\n",
    "bins = np.linspace(global_min, global_max, 20)  # Change 50 to the number of bins you want\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# Plot for Cluster 0\n",
    "axs[0].hist(cluster_0_latencies, bins=bins, color=custom_colors.get('Cluster 0', 'b'))\n",
    "axs[0].set_title(\"Latencies for Cluster 0\")\n",
    "axs[0].set_xlabel(\"Latency (frames)\")\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot for Cluster -1\n",
    "axs[1].hist(cluster_neg1_latencies, bins=bins, color=custom_colors.get('Cluster -1', 'r'))\n",
    "axs[1].set_title(\"Latencies for Cluster -1\")\n",
    "axs[1].set_xlabel(\"Latency (frames)\")\n",
    "axs[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a13152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to collect latencies for each cluster\n",
    "cluster_0_latencies = []\n",
    "cluster_neg1_latencies = []\n",
    "\n",
    "# Loop through the trials to collect data\n",
    "for stimulus_type, stimulus_trials in trials.items():\n",
    "    for trial in stimulus_trials:\n",
    "        F_stim = trial['metadata'].get('F_stim', None)\n",
    "        valid_event_times = trial.get('valid_event_times', [])\n",
    "        event_clusters = trial.get('event_clusters', [])\n",
    "\n",
    "        if F_stim is not None:\n",
    "            for i, event_time in enumerate(valid_event_times):\n",
    "                latency = event_time - F_stim\n",
    "                \n",
    "                if event_clusters[i] == 0:\n",
    "                    cluster_0_latencies.append(latency)\n",
    "                elif event_clusters[i] == -1:\n",
    "                    cluster_neg1_latencies.append(latency)\n",
    "\n",
    "# Determine global min and max latencies for plotting\n",
    "global_min = 0\n",
    "global_max = 400\n",
    "\n",
    "# Create uniform bins using global min and max\n",
    "bins = np.linspace(global_min, global_max, 20)  # Change 50 to the number of bins you want\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# Plot for Cluster 0\n",
    "axs[0].hist(cluster_0_latencies, bins=bins, color=custom_colors.get('Cluster 0', 'b'))\n",
    "axs[0].set_title(\"Latencies for Cluster 0\")\n",
    "axs[0].set_xlabel(\"Latency (frames)\")\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot for Cluster -1\n",
    "axs[1].hist(cluster_neg1_latencies, bins=bins, color=custom_colors.get('Cluster -1', 'r'))\n",
    "axs[1].set_title(\"Latencies for Cluster -1\")\n",
    "axs[1].set_xlabel(\"Latency (frames)\")\n",
    "axs[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febdaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a list to collect F_stim values\n",
    "all_F_stims = []\n",
    "\n",
    "# Loop through the trials to collect F_stim values\n",
    "for stimulus_type, stimulus_trials in trials.items():\n",
    "    for trial in stimulus_trials:\n",
    "        F_stim = trial['metadata'].get('F_stim', None)\n",
    "        if F_stim is not None:\n",
    "            all_F_stims.append(F_stim)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(all_F_stims, bins=15, color='b')  # Change 50 to the number of bins you want\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Histogram of F_stim Values\")\n",
    "plt.xlabel(\"F_stim (frames)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
